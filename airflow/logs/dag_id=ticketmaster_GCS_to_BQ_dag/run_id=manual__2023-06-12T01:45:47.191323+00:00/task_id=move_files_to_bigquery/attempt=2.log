[2023-06-12T01:48:47.134+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ticketmaster_GCS_to_BQ_dag.move_files_to_bigquery manual__2023-06-12T01:45:47.191323+00:00 [queued]>
[2023-06-12T01:48:47.140+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ticketmaster_GCS_to_BQ_dag.move_files_to_bigquery manual__2023-06-12T01:45:47.191323+00:00 [queued]>
[2023-06-12T01:48:47.140+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-06-12T01:48:47.141+0000] {taskinstance.py:1289} INFO - Starting attempt 2 of 3
[2023-06-12T01:48:47.142+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-06-12T01:48:47.151+0000] {taskinstance.py:1309} INFO - Executing <Task(GCSToBigQueryOperator): move_files_to_bigquery> on 2023-06-12 01:45:47.191323+00:00
[2023-06-12T01:48:47.157+0000] {standard_task_runner.py:55} INFO - Started process 372 to run task
[2023-06-12T01:48:47.162+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ticketmaster_GCS_to_BQ_dag', 'move_files_to_bigquery', 'manual__2023-06-12T01:45:47.191323+00:00', '--job-id', '232', '--raw', '--subdir', 'DAGS_FOLDER/gcs_to_bigquery.py', '--cfg-path', '/tmp/tmp177qxdg8']
[2023-06-12T01:48:47.165+0000] {standard_task_runner.py:83} INFO - Job 232: Subtask move_files_to_bigquery
[2023-06-12T01:48:47.249+0000] {task_command.py:389} INFO - Running <TaskInstance: ticketmaster_GCS_to_BQ_dag.move_files_to_bigquery manual__2023-06-12T01:45:47.191323+00:00 [running]> on host 04f5a8a63034
[2023-06-12T01:48:47.315+0000] {taskinstance.py:1518} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=nburkett
AIRFLOW_CTX_DAG_ID=ticketmaster_GCS_to_BQ_dag
AIRFLOW_CTX_TASK_ID=move_files_to_bigquery
AIRFLOW_CTX_EXECUTION_DATE=2023-06-12T01:45:47.191323+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-06-12T01:45:47.191323+00:00
[2023-06-12T01:48:47.317+0000] {crypto.py:83} WARNING - empty cryptography key - values will not be stored encrypted.
[2023-06-12T01:48:47.318+0000] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-06-12T01:48:47.319+0000] {gcs_to_bigquery.py:382} INFO - Using existing BigQuery table for storing data...
[2023-06-12T01:48:47.362+0000] {gcs_to_bigquery.py:386} INFO - Executing: {'load': {'autodetect': True, 'createDisposition': 'CREATE_IF_NEEDED', 'destinationTable': {'projectId': 'global-maxim-338114', 'datasetId': 'twitter_kafka_pyspark_test', 'tableId': 'ticketmaster_***_stg'}, 'sourceFormat': 'CSV', 'sourceUris': ['gs://dtc_data_lake_global-maxim-338114/*.parquet'], 'writeDisposition': 'WRITE_APPEND', 'ignoreUnknownValues': False, 'destinationTableProperties': {'description': 'Hourly data pipeline to generate dims and facts for streamify'}, 'skipLeadingRows': None, 'fieldDelimiter': ',', 'quote': None, 'allowQuotedNewlines': False, 'encoding': 'UTF-8'}}
[2023-06-12T01:48:47.366+0000] {bigquery.py:1546} INFO - Inserting job ***_ticketmaster_GCS_to_BQ_dag_move_files_to_bigquery_2023_06_12T01_45_47_191323_00_00_e65b1375b62f5be465b9f02ad4e3951f
[2023-06-12T01:48:47.941+0000] {taskinstance.py:1776} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 445, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/job/base.py", line 728, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/future/polling.py", line 137, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Description provided for destination table does not match existing description. To update table description for an existing table, use tables.update. Existing table description is "", but provided table description is "Hourly data pipeline to generate dims and facts for streamify"
[2023-06-12T01:48:47.952+0000] {taskinstance.py:1332} INFO - Marking task as UP_FOR_RETRY. dag_id=ticketmaster_GCS_to_BQ_dag, task_id=move_files_to_bigquery, execution_date=20230612T014547, start_date=20230612T014847, end_date=20230612T014847
[2023-06-12T01:48:47.960+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 232 for task move_files_to_bigquery (400 Description provided for destination table does not match existing description. To update table description for an existing table, use tables.update. Existing table description is "", but provided table description is "Hourly data pipeline to generate dims and facts for streamify"; 372)
[2023-06-12T01:48:47.988+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-06-12T01:48:48.003+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
