{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na_cols(df):\n",
    "    # Calculate the threshold for NaN values\n",
    "    threshold = len(df) * 0.75\n",
    "    percent_null = df.isnull().sum() / len(df) * 100\n",
    "\n",
    "    cols_to_drop = percent_null[percent_null > 50].index\n",
    "    num_cols_dropped = len(df.columns) - len(cols_to_drop)\n",
    "    print(f'Dropping {num_cols_dropped} cols, columns to drop: {cols_to_drop.tolist()}')\n",
    "    # Drop columns exceeding the threshold\n",
    "    df = df.drop(cols_to_drop,axis=1 )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "\n",
    "# Set up API credentials and parameters\n",
    "api_key = '16AI00EAgNeGSZqgQKrLc0GUf8fVhDaa'\n",
    "base_url = 'https://app.ticketmaster.com/discovery/v2/events.json'\n",
    "city = 'Austin'\n",
    "state = 'TX'\n",
    "date_range = f\"{date.today()},{date.today() + timedelta(days=90)}\"\n",
    "params = {\n",
    "    'apikey': api_key,\n",
    "    'city': city,\n",
    "    'stateCode': state,\n",
    "    'start_date': datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    'end_date': (datetime.now() + timedelta(days=10)).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    'stateCode':'TX',\n",
    "    'preferredCountry':'us',\n",
    "    'size': 50 , # Adjust the number of results per page as per your needs\n",
    "    # 'page': 0\n",
    "}\n",
    "\n",
    "# Send API requests and fetch all pages of data\n",
    "all_events = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    params['page'] = page\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if '_embedded' in data and 'events' in data['_embedded']:\n",
    "        act_page = data['page']['number']\n",
    "        print(f'Current Actual Page: {act_page}')\n",
    "        events = data['_embedded']['events']\n",
    "        all_events.extend(events)\n",
    "        \n",
    "        if 'page' in data and 'totalPages' in data['page'] and data['page']['number'] < data['page']['totalPages'] - 1:\n",
    "            page += 1\n",
    "            print(f'Incremented Page # to: {page}')\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        print(f'Page #: {page}, Incorrect response format, terminating')\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(all_events)\n",
    "# Drop columns that are mostly NaNs\n",
    "df = drop_na_cols(df)\n",
    "\n",
    "#Drop Unnecessary Columns\n",
    "df2 = df.drop(columns=['images','seatmap','accessibility','ageRestrictions','test','_links'])\n",
    "\n",
    "\n",
    "# Print the dataframe\n",
    "# print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df\n",
    "df2\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten_json(nested_json, exclude=['']):\n",
    "    \"\"\"Flatten json object with nested keys into a single level.\n",
    "        Args:\n",
    "            nested_json: A nested json object.\n",
    "            exclude: Keys to exclude from output.\n",
    "        Returns:\n",
    "            The flattened json object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name='', exclude=exclude):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                if a not in exclude:\n",
    "                    flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out\n",
    "\n",
    "\n",
    "def flatten_and_drop(original_df, cols_to_flatten):\n",
    "    merged_df = original_df.copy()  # Initialize merged_df with the original DataFrame\n",
    "\n",
    "    for col in cols_to_flatten:\n",
    "        # Apply flatten_json function to every row in the DataFrame\n",
    "        flattened_data = original_df[col].apply(flatten_json)\n",
    "\n",
    "        # Convert the flattened data to a DataFrame\n",
    "        flattened_df = pd.DataFrame.from_records(flattened_data)\n",
    "\n",
    "        # Calculate the threshold for NaN values\n",
    "        threshold = len(flattened_df) * 0.75\n",
    "\n",
    "        # Drop columns exceeding the threshold\n",
    "        flattened_df = flattened_df.dropna(axis=1, thresh=threshold)\n",
    "\n",
    "        # Concatenate the flattened DataFrame with the merged DataFrame\n",
    "        merged_df = pd.concat([merged_df, flattened_df], axis=1)\n",
    "\n",
    "    #combine list of columns to drop \n",
    "    prefix = 'attractions_0_images'\n",
    "\n",
    "    #Combine to drop all at once \n",
    "\n",
    "    columns_to_delete = cols_to_flatten + merged_df.columns[merged_df.columns.str.startswith(prefix)].tolist()\n",
    "    # Drop the original columns from the DataFrame\n",
    "    merged_df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "    pattern = '_0'\n",
    "    #get col names in list \n",
    "    column_names = merged_df.columns.tolist()\n",
    "    #use list comprehension to replace pattern in col names\n",
    "    new_column_names = [name.replace(pattern, '') for name in column_names]\n",
    "    #update the col names in the dataframe\n",
    "    merged_df.columns = new_column_names\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "# Apply the flatten_and_drop function to your DataFrame\n",
    "df3 = flatten_and_drop(df2, ['_embedded','dates','sales','classifications','priceRanges'])\n",
    "\n",
    "##add the price ranges back in  \n",
    "price_data = df2['priceRanges'].apply(flatten_json)\n",
    "price_df = pd.DataFrame.from_records(price_data)\n",
    "\n",
    "\n",
    "df3 = pd.concat([df3, price_df], axis=1)\n",
    "\n",
    "pattern = '0_'\n",
    "column_names = df3.columns.tolist()\n",
    "new_column_names = [name.replace(pattern, '') for name in column_names]\n",
    "df3.columns = new_column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decide to only keep these \n",
    "keep_cols = ['name',\n",
    "'type',\n",
    "'id',\n",
    "'url',\n",
    "'venues_name',\n",
    "'venues_id',\n",
    "'venues_postalCode',\n",
    "'venues_timezone',\n",
    "'venues_city_name',\n",
    "'venues_state_name',\n",
    "'venues_state_stateCode',\n",
    "'venues_country_name',\n",
    "'venues_country_countryCode',\n",
    "'venues_address_line1',\n",
    "'venues_location_longitude',\n",
    "'venues_location_latitude',\n",
    "'attractions_name',\n",
    "'attractions_type',\n",
    "'attractions_id',\n",
    "'attractions_url',\n",
    "'attractions_classifications_segment_id',\n",
    "'attractions_classifications_segment_name',\n",
    "'attractions_classifications_genre_id',\n",
    "'attractions_classifications_genre_name',\n",
    "'attractions_classifications_subGenre_id',\n",
    "'attractions_classifications_subGenre_name',\n",
    "'start_localDate',\n",
    "'status_code',\n",
    "'start_localTime',\n",
    "'currency',\n",
    "'min',\n",
    "'max',]\n",
    "\n",
    "df3 = df3[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a dictionary to map old column names to new column names\n",
    "column_mapping = {\n",
    "    'name': 'event_name',\n",
    "    'type': 'event_type',\n",
    "    'id': 'event_id',\n",
    "    'url': 'event_url',\n",
    "    'venues_name': 'venue_name',\n",
    "    'venues_id': 'venue_id',\n",
    "    'venues_postalCode': 'venue_zipcode',\n",
    "    'venues_timezone': 'venues_timezone',\n",
    "    'venues_city_name': 'venue_city',\n",
    "    'venues_state_name': 'venue_state_full',\n",
    "    'venues_state_stateCode': 'venue_state_short',\n",
    "    'venues_country_name': 'venue_country_name',\n",
    "    'venues_country_countryCode': 'venue_country_short',\n",
    "    'venues_address_line1': 'venue_address',\n",
    "    'venues_location_longitude': 'venue_longitude',\n",
    "    'venues_location_latitude': 'venue_latitude',\n",
    "    'attractions_name': 'attraction_name',\n",
    "    'attractions_type': 'attraction_type',\n",
    "    'attractions_id': 'attraction_id',\n",
    "    'attractions_url': 'attraction_url',\n",
    "    'attractions_classifications_segment_id': 'attraction_segment_id',\n",
    "    'attractions_classifications_segment_name': 'attraction_segment_name',\n",
    "    'attractions_classifications_genre_id': 'attraction_genre_id',\n",
    "    'attractions_classifications_genre_name': 'attraction_genre_name',\n",
    "    'attractions_classifications_subGenre_id': 'attraction_subgenre_id',\n",
    "    'attractions_classifications_subGenre_name': 'attraction_subgenre_name',\n",
    "    'start_localDate': 'event_start_date',\n",
    "    'status_code': 'ticket_status',\n",
    "    'start_localTime': 'event_start_time',\n",
    "    'currency': 'currency',\n",
    "    'min': 'min_price',\n",
    "    'max': 'max_price'\n",
    "}\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "df3 = df3.rename(columns=column_mapping)\n",
    "\n",
    "df3.to_csv('/Users/nicburkett/Downloads/after_transformation.csv')\n",
    "df3.head()\n",
    "\n",
    "##reset the index so we can convert to json \n",
    "df3.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET UP THE KAFKA PRODUCER \n",
    "import json \n",
    "import random \n",
    "from datetime import datetime\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "topic_name = 'twitter'\n",
    "# Messages will be serialized as JSON \n",
    "def serializer(message):\n",
    "    return json.dumps(message).encode('utf-8')\n",
    "\n",
    "# Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=serializer\n",
    ")\n",
    "\n",
    "## Clean out the producer \n",
    "producer.flush() \n",
    "\n",
    "# Convert each row to JSON and send as message to Kafka\n",
    "for _, row in df3.iterrows():\n",
    "    # Convert the row to a dictionary with column names as keys\n",
    "    data = row.to_dict()\n",
    "\n",
    "    # Include the column names in the dictionary\n",
    "    message = {col: data[col] for col in df3.columns}\n",
    "\n",
    "    # Print the JSON message\n",
    "    print(message)\n",
    "    \n",
    "    # Send the JSON message to the Kafka topic\n",
    "    producer.send(topic_name, message)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Close the Kafka producer\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## empty out the `twitter` topic\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### \n",
    "######### END \n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.indexes.base.Index'>\n"
     ]
    }
   ],
   "source": [
    "#### Format the columns\n",
    "# Format the start date and time\n",
    "df['start.dateTime'] = pd.to_datetime(df['start.dateTime'])\n",
    "df['start.dateTime'] = df['start.dateTime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Austin\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'apikey': api_key,\n",
    "    'city': 'dallas',\n",
    "    'stateCode': 'tx',\n",
    "    'countryCode': 'US',\n",
    "    'size': 50 , # Adjust the number of results per page as per your needs\n",
    "}\n",
    "\n",
    "print(f'testing {city}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "\n",
    "# Set up API credentials and parameters\n",
    "api_key = '16AI00EAgNeGSZqgQKrLc0GUf8fVhDaa'\n",
    "base_url = 'https://app.ticketmaster.com/discovery/v2/events.json'\n",
    "\n",
    "params = {\n",
    "    'apikey': api_key,\n",
    "    'city': 'Austin',\n",
    "    'stateCode': 'TX',\n",
    "    'countryCode':'US',\n",
    "    'size': 50 , # Adjust the number of results per page as per your needs\n",
    "    # 'page': 0\n",
    "}\n",
    "\n",
    "\n",
    "# Send API requests and fetch all pages of data\n",
    "all_events = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    params['page'] = page\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if '_embedded' in data and 'events' in data['_embedded']:\n",
    "        act_page = data['page']['number']\n",
    "        print(f'Current Actual Page: {act_page}')\n",
    "        events = data['_embedded']['events']\n",
    "        all_events.extend(events)\n",
    "        \n",
    "        if 'page' in data and 'totalPages' in data['page'] and data['page']['number'] < data['page']['totalPages'] - 1:\n",
    "            page += 1\n",
    "            print(f'Incremented Page # to: {page}')\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        print(f'Page #: {page}, Incorrect response format, terminating')\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(all_events)\n",
    "\n",
    "price_copy = df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 11 cols, columns to drop: ['outlets', 'promoter', 'promoters', 'ticketLimit', 'products', 'info', 'pleaseNote', 'doorsTimes', 'description']\n",
      "Now dataframe has 11 columns and 586 rows\n"
     ]
    }
   ],
   "source": [
    "# print(f'Created dataframe showing all events in {city},{stateCode} with {len(df.columns)} columns and {len(df.index)} rows')\n",
    "# Drop selected cols & columns that are mostly NaNs\n",
    "df = df.drop(columns=['images','seatmap','accessibility','ageRestrictions','test','_links'])\n",
    "df = drop_na_cols(df)\n",
    "\n",
    "#Drop Unnecessary Columns\n",
    "print(f'Now dataframe has {len(df.columns)} columns and {len(df.index)} rows')\n",
    "\n",
    "# Apply the flatten_and_drop function to your DataFrame\n",
    "df = flatten_and_drop(df, ['_embedded','dates','sales','classifications','priceRanges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now dataframe has 74 columns and 586 rows\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'_embedded'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '_embedded'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNow dataframe has \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(df2\u001b[39m.\u001b[39mcolumns)\u001b[39m}\u001b[39;00m\u001b[39m columns and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(df2\u001b[39m.\u001b[39mindex)\u001b[39m}\u001b[39;00m\u001b[39m rows\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Apply the flatten_and_drop function to your DataFrame\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df2 \u001b[39m=\u001b[39m flatten_and_drop(df2, [\u001b[39m'\u001b[39;49m\u001b[39m_embedded\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mdates\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39msales\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mclassifications\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mpriceRanges\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      8\u001b[0m \u001b[39m##add the price ranges back in  \u001b[39;00m\n\u001b[1;32m      9\u001b[0m price_data \u001b[39m=\u001b[39m price_copy[\u001b[39m'\u001b[39m\u001b[39mpriceRanges\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(flatten_json)\n",
      "Cell \u001b[0;32mIn[19], line 33\u001b[0m, in \u001b[0;36mflatten_and_drop\u001b[0;34m(original_df, cols_to_flatten)\u001b[0m\n\u001b[1;32m     29\u001b[0m merged_df \u001b[39m=\u001b[39m original_df\u001b[39m.\u001b[39mcopy()  \u001b[39m# Initialize merged_df with the original DataFrame\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m cols_to_flatten:\n\u001b[1;32m     32\u001b[0m     \u001b[39m# Apply flatten_json function to every row in the DataFrame\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     flattened_data \u001b[39m=\u001b[39m original_df[col]\u001b[39m.\u001b[39mapply(flatten_json)\n\u001b[1;32m     35\u001b[0m     \u001b[39m# Convert the flattened data to a DataFrame\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     flattened_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(flattened_data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/pandas/core/frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '_embedded'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Drop Unnecessary Columns\n",
    "print(f'Now dataframe has {len(df2.columns)} columns and {len(df2.index)} rows')\n",
    "\n",
    "\n",
    "# Apply the flatten_and_drop function to your DataFrame\n",
    "df2 = flatten_and_drop(df2, ['_embedded','dates','sales','classifications','priceRanges'])\n",
    "\n",
    "##add the price ranges back in  \n",
    "price_data = price_copy['priceRanges'].apply(flatten_json)\n",
    "price_df = pd.DataFrame.from_records(price_data)\n",
    "\n",
    "\n",
    "df2 = pd.concat([df2, price_df], axis=1)\n",
    "\n",
    "pattern = '0_'\n",
    "column_names = df2.columns.tolist()\n",
    "new_column_names = [name.replace(pattern, '') for name in column_names]\n",
    "df2.columns = new_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'type', 'id', 'test', 'url', 'locale', 'images', 'seatmap',\n",
       "       '_links', 'accessibility', 'ageRestrictions', 'ticketing',\n",
       "       'venues_name', 'venues_type', 'venues_id', 'venues_test',\n",
       "       'venues_locale', 'venues_postalCode', 'venues_timezone',\n",
       "       'venues_city_name', 'venues_state_name', 'venues_state_stateCode',\n",
       "       'venues_country_name', 'venues_country_countryCode',\n",
       "       'venues_address_line1', 'venues_location_longitude',\n",
       "       'venues_location_latitude', 'venues_dmas_id',\n",
       "       'venues_upcomingEvents__total', 'venues_upcomingEvents__filtered',\n",
       "       'venues_upcomingEventsBy_country_US', 'venues__links_self_href',\n",
       "       'attractions_name', 'attractions_type', 'attractions_id',\n",
       "       'attractions_test', 'attractions_url', 'attractions_locale',\n",
       "       'attractions_classifications_primary',\n",
       "       'attractions_classifications_segment_id',\n",
       "       'attractions_classifications_segment_name',\n",
       "       'attractions_classifications_genre_id',\n",
       "       'attractions_classifications_genre_name',\n",
       "       'attractions_classifications_subGenre_id',\n",
       "       'attractions_classifications_subGenre_name',\n",
       "       'attractions_classifications_type_id',\n",
       "       'attractions_classifications_type_name',\n",
       "       'attractions_classifications_subType_id',\n",
       "       'attractions_classifications_subType_name',\n",
       "       'attractions_classifications_family',\n",
       "       'attractions_upcomingEvents__total', 'attractions_upcomingEvents_tmr',\n",
       "       'attractions_upcomingEvents__filtered',\n",
       "       'attractions_upcomingEventsBy_country_US',\n",
       "       'attractions_upcomingEventsBy_source_tmr',\n",
       "       'attractions__links_self_href',\n",
       "       'attractions_upcomingEvents_ticketmaster',\n",
       "       'attractions_upcomingEventsBy_source_ticketmaster', 'start_localDate',\n",
       "       'start_dateTBD', 'start_dateTBA', 'start_timeTBA',\n",
       "       'start_noSpecificTime', 'status_code', 'spanMultipleDays',\n",
       "       'start_localTime', 'start_dateTime', 'public_startDateTime',\n",
       "       'public_startTBD', 'public_startTBA', 'public_endDateTime', '0_primary',\n",
       "       '0_segment_id', '0_segment_name', '0_genre_id', '0_genre_name',\n",
       "       '0_subGenre_id', '0_subGenre_name', '0_family'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the pattern to replace\n",
    "pattern = '_0'\n",
    "\n",
    "# Get the current column names\n",
    "column_names = df3.columns.tolist()\n",
    "\n",
    "# Replace the pattern in column names\n",
    "new_column_names = [name.replace(pattern, '') for name in column_names]\n",
    "\n",
    "# Update the column names in the DataFrame\n",
    "df3.columns = new_column_names\n",
    "\n",
    "# Output the DataFrame with updated column names\n",
    "(df3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['attractions_0_images_0_ratio', 'attractions_0_images_0_url',\n",
       "       'attractions_0_images_0_width', 'attractions_0_images_0_height',\n",
       "       'attractions_0_images_0_fallback', 'attractions_0_images_1_ratio',\n",
       "       'attractions_0_images_1_url', 'attractions_0_images_1_width',\n",
       "       'attractions_0_images_1_height', 'attractions_0_images_1_fallback',\n",
       "       'attractions_0_images_2_ratio', 'attractions_0_images_2_url',\n",
       "       'attractions_0_images_2_width', 'attractions_0_images_2_height',\n",
       "       'attractions_0_images_2_fallback', 'attractions_0_images_3_ratio',\n",
       "       'attractions_0_images_3_url', 'attractions_0_images_3_width',\n",
       "       'attractions_0_images_3_height', 'attractions_0_images_3_fallback',\n",
       "       'attractions_0_images_4_ratio', 'attractions_0_images_4_url',\n",
       "       'attractions_0_images_4_width', 'attractions_0_images_4_height',\n",
       "       'attractions_0_images_4_fallback', 'attractions_0_images_5_ratio',\n",
       "       'attractions_0_images_5_url', 'attractions_0_images_5_width',\n",
       "       'attractions_0_images_5_height', 'attractions_0_images_5_fallback',\n",
       "       'attractions_0_images_6_ratio', 'attractions_0_images_6_url',\n",
       "       'attractions_0_images_6_width', 'attractions_0_images_6_height',\n",
       "       'attractions_0_images_6_fallback', 'attractions_0_images_7_ratio',\n",
       "       'attractions_0_images_7_url', 'attractions_0_images_7_width',\n",
       "       'attractions_0_images_7_height', 'attractions_0_images_7_fallback',\n",
       "       'attractions_0_images_8_ratio', 'attractions_0_images_8_url',\n",
       "       'attractions_0_images_8_width', 'attractions_0_images_8_height',\n",
       "       'attractions_0_images_8_fallback', 'attractions_0_images_9_ratio',\n",
       "       'attractions_0_images_9_url', 'attractions_0_images_9_width',\n",
       "       'attractions_0_images_9_height', 'attractions_0_images_9_fallback'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'attractions_0_images'\n",
    "columns_to_delete = df3.columns[df3.columns.str.startswith(prefix)]\n",
    "columns_to_delete\n",
    "\n",
    "\n",
    "def delete_columns(df,prefix):\n",
    "    columns_to_delete = df.columns[df.columns.str.startswith(prefix)]\n",
    "    df.drop(columns=columns_to_delete, inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
