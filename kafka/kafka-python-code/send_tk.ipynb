{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def drop_na_cols(df):\n",
    "    \"\"\"Drops columns that are 75% NaN.\n",
    "    Args:\n",
    "        dataframe: Dataframe you want to drop columns from.\n",
    "    Returns:\n",
    "        dataframe: Dataframe with dropped columns.\n",
    "    \"\"\"\n",
    "    threshold = len(df) * 0.75\n",
    "    percent_null = df.isnull().sum() / len(df) * 100\n",
    "\n",
    "    cols_to_drop = percent_null[percent_null > 0.5].index\n",
    "    num_cols_dropped = len(df.columns) - len(cols_to_drop)\n",
    "    print(f'Dropping {num_cols_dropped} cols, columns to drop: {cols_to_drop.tolist()}')\n",
    "    # Drop columns exceeding the threshold\n",
    "    df = df.drop(cols_to_drop,axis=1 )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def flatten_json(nested_json, exclude=['']):\n",
    "    \"\"\"Flatten json object with nested keys into a single level.\n",
    "        Args:\n",
    "            nested_json: A nested json object.\n",
    "            exclude: Keys to exclude from output.\n",
    "        Returns:\n",
    "            The flattened json object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name='', exclude=exclude):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                if a not in exclude:\n",
    "                    flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "    flatten(nested_json)\n",
    "    return out\n",
    "\n",
    "\n",
    "def flatten_and_drop(original_df, cols_to_flatten):\n",
    "    \"\"\"Flattens and then drops columns from a DataFrame.\n",
    "    Args:\n",
    "        dataframe: Dataframe you want to drop columns from.\n",
    "        cols_to_flatten: List of columns to flatten and drop NaNs from.\n",
    "    Returns:\n",
    "        dataframe: Dataframe with dropped columns.\n",
    "    \"\"\"\n",
    "    merged_df = original_df.copy()  # Initialize merged_df with the original DataFrame\n",
    "\n",
    "    for col in cols_to_flatten:\n",
    "        # Apply flatten_json function to every row in the DataFrame\n",
    "        flattened_data = original_df[col].apply(flatten_json)\n",
    "\n",
    "        # Convert the flattened data to a DataFrame\n",
    "        flattened_df = pd.DataFrame.from_records(flattened_data)\n",
    "\n",
    "        # Calculate the threshold for NaN values\n",
    "        threshold = len(flattened_df) * 0.75\n",
    "\n",
    "        # Drop columns exceeding the threshold\n",
    "        flattened_df = flattened_df.dropna(axis=1, thresh=threshold)\n",
    "\n",
    "        # Concatenate the flattened DataFrame with the merged DataFrame\n",
    "        merged_df = pd.concat([merged_df, flattened_df], axis=1)\n",
    "\n",
    "    #combine list of columns to drop \n",
    "    prefix = 'attractions_0_images'\n",
    "\n",
    "    #Combine to drop all at once \n",
    "    columns_to_delete = cols_to_flatten + merged_df.columns[merged_df.columns.str.startswith(prefix)].tolist()\n",
    "    # Drop the original columns from the DataFrame\n",
    "    merged_df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "    pattern = '_0'\n",
    "    #get col names in list \n",
    "    column_names = merged_df.columns.tolist()\n",
    "    #use list comprehension to replace pattern in col names\n",
    "    new_column_names = [name.replace(pattern, '') for name in column_names]\n",
    "    #update the col names in the dataframe\n",
    "    merged_df.columns = new_column_names\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "def fetch_ticketmaster_data(city:str, stateCode: str, countryCode:str ) -> pd.DataFrame :\n",
    "    \"\"\"Fetches data from the Ticketmaster API.\n",
    "    Args:\n",
    "        city: City you want to search for events in. \"Austin\"\n",
    "        stateCode: State you want to search for events in. \"TX\"\n",
    "        countryCode: Country you want to search for events in. \"US\"\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Dataframe with data from the Ticketmaster API.\n",
    "    \"\"\"\n",
    "    # Set up API credentials and parameters\n",
    "    base_url = 'https://app.ticketmaster.com/discovery/v2/events.json'\n",
    "    params = {\n",
    "        'apikey': '16AI00EAgNeGSZqgQKrLc0GUf8fVhDaa',\n",
    "        'city': city,\n",
    "        'stateCode': stateCode,\n",
    "        'countryCode': countryCode,\n",
    "        'size': 50 , # Adjust the number of results per page as per your needs\n",
    "    }\n",
    "    # Send API requests and fetch all pages of data\n",
    "    all_events = []\n",
    "    page = 0\n",
    "\n",
    "    while True:\n",
    "        params['page'] = page\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if '_embedded' in data and 'events' in data['_embedded']:\n",
    "            act_page = data['page']['number']\n",
    "            print(f'Current Actual Page: {act_page}')\n",
    "            events = data['_embedded']['events']\n",
    "            all_events.extend(events)\n",
    "            \n",
    "            if 'page' in data and 'totalPages' in data['page'] and data['page']['number'] < data['page']['totalPages'] - 1:\n",
    "                page += 1\n",
    "                # print(f'Incremented Page # to: {page}')\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print(f'Page #: {page}, Incorrect response format, terminating')\n",
    "            print(data)\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame(all_events)\n",
    "    print(f'Created dataframe showing all events in {city},{stateCode} with {len(df.columns)} columns and {len(df.index)} rows')\n",
    "    price_copy = df.copy()\n",
    "    # Drop selected cols & columns that are mostly NaNs\n",
    "    # df = df.drop(columns=['images','seatmap','accessibility','ageRestrictions','test','_links'])\n",
    "    # df = drop_na_cols(df)\n",
    "\n",
    "    #Drop Unnecessary Columns\n",
    "    print(f'Now dataframe has {len(df.columns)} columns and {len(df.index)} rows')\n",
    "\n",
    "    # Apply the flatten_and_drop function to your DataFrame\n",
    "    df = flatten_and_drop(df, ['_embedded','dates','sales','classifications','priceRanges'])\n",
    "\n",
    "    ##add the price ranges back in  \n",
    "    price_data = price_copy['priceRanges'].apply(flatten_json)\n",
    "    price_df = pd.DataFrame.from_records(price_data)\n",
    "\n",
    "\n",
    "    df = pd.concat([df, price_df], axis=1)\n",
    "\n",
    "    pattern = '0_'\n",
    "    column_names = df.columns.tolist()\n",
    "    new_column_names = [name.replace(pattern, '') for name in column_names]\n",
    "    df.columns = new_column_names\n",
    "\n",
    "    print(f'Now dataframe has {len(df.columns)} columns and {len(df.index)} rows')\n",
    "    ### decide to only keep these \n",
    "    keep_cols = ['name',\n",
    "    'type',\n",
    "    'id',\n",
    "    'url',\n",
    "    'venues_name',\n",
    "    'venues_id',\n",
    "    'venues_postalCode',\n",
    "    'venues_timezone',\n",
    "    'venues_city_name',\n",
    "    'venues_state_name',\n",
    "    'venues_state_stateCode',\n",
    "    'venues_country_name',\n",
    "    'venues_country_countryCode',\n",
    "    'venues_address_line1',\n",
    "    'venues_location_longitude',\n",
    "    'venues_location_latitude',\n",
    "    'attractions_name',\n",
    "    'attractions_type',\n",
    "    'attractions_id',\n",
    "    'attractions_url',\n",
    "    'attractions_classifications_segment_id',\n",
    "    'attractions_classifications_segment_name',\n",
    "    'attractions_classifications_genre_id',\n",
    "    'attractions_classifications_genre_name',\n",
    "    'attractions_classifications_subGenre_id',\n",
    "    'attractions_classifications_subGenre_name',\n",
    "    'start_localDate',\n",
    "    'status_code',\n",
    "    'start_localTime',\n",
    "    'currency',\n",
    "    'min',\n",
    "    'max',]\n",
    "\n",
    "    df = df[keep_cols]\n",
    "\n",
    "\n",
    "    # Define a dictionary to map old column names to new column names\n",
    "    column_mapping = {\n",
    "        'name': 'event_name',\n",
    "        'type': 'event_type',\n",
    "        'id': 'event_id',\n",
    "        'url': 'event_url',\n",
    "        'venues_name': 'venue_name',\n",
    "        'venues_id': 'venue_id',\n",
    "        'venues_postalCode': 'venue_zipcode',\n",
    "        'venues_timezone': 'venues_timezone',\n",
    "        'venues_city_name': 'venue_city',\n",
    "        'venues_state_name': 'venue_state_full',\n",
    "        'venues_state_stateCode': 'venue_state_short',\n",
    "        'venues_country_name': 'venue_country_name',\n",
    "        'venues_country_countryCode': 'venue_country_short',\n",
    "        'venues_address_line1': 'venue_address',\n",
    "        'venues_location_longitude': 'venue_longitude',\n",
    "        'venues_location_latitude': 'venue_latitude',\n",
    "        'attractions_name': 'attraction_name',\n",
    "        'attractions_type': 'attraction_type',\n",
    "        'attractions_id': 'attraction_id',\n",
    "        'attractions_url': 'attraction_url',\n",
    "        'attractions_classifications_segment_id': 'attraction_segment_id',\n",
    "        'attractions_classifications_segment_name': 'attraction_segment_name',\n",
    "        'attractions_classifications_genre_id': 'attraction_genre_id',\n",
    "        'attractions_classifications_genre_name': 'attraction_genre_name',\n",
    "        'attractions_classifications_subGenre_id': 'attraction_subgenre_id',\n",
    "        'attractions_classifications_subGenre_name': 'attraction_subgenre_name',\n",
    "        'start_localDate': 'event_start_date',\n",
    "        'status_code': 'ticket_status',\n",
    "        'start_localTime': 'event_start_time',\n",
    "        'currency': 'currency',\n",
    "        'min': 'min_price',\n",
    "        'max': 'max_price'\n",
    "    }\n",
    "\n",
    "    # Rename the columns using the dictionary\n",
    "    df = df.rename(columns=column_mapping)\n",
    "\n",
    "    ##reset the index so we can convert to json \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(f'Dataframe for {city},{stateCode} loaded. Final size: {len(df.columns)} columns, {len(df.index)} rows')\n",
    "    return df\n",
    "\n",
    "\n",
    "# df = fetch_ticketmaster_data('Dallas','TX','US')\n",
    "# df.to_csv('/Users/nicburkett/Downloads/dallas_data.csv')\n",
    "# df.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticketmaster_api_fetch import fetch_ticketmaster_data\n",
    "## INPUT \n",
    "df = fetch_ticketmaster_data('Houston','TX','US')\n",
    "# df.head()\n",
    "df.to_csv('/Users/nicburkett/Downloads/houston_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
