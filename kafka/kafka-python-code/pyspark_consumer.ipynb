{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, udf, col, when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticketmaster\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "topic_name = os.getenv('kafka_topic_name')\n",
    "\n",
    "# spark_path = findspark.find()\n",
    "# print(spark_path)\n",
    "print(topic_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = os.getenv('gcp_credentials_path')\n",
    "\n",
    "# Spark Config\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('TwitterSentimentAnalysis') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
    "    .set(\"spark.jars\", \"gs://path/to/spark-bigquery-latest.jar,gs://path/to/google-cloud-bigquery-latest.jar,/path/to/local-jar-file.jar\") \\\n",
    "    .set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\\\n",
    "    .set(\"spark.jars\", \"gcs-connector-hadoop3-2.2.5.jar\") \n",
    "\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "\n",
    "# Spark Context\n",
    "# sc = spark.sparkContext\n",
    "# sc.setLogLevel('ERROR')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:00:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71d3a546-2d9e-4f76-a6b9-bc85d9902881--2001384727-driver-0-1, groupId=spark-kafka-source-71d3a546-2d9e-4f76-a6b9-bc85d9902881--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:00:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71d3a546-2d9e-4f76-a6b9-bc85d9902881--2001384727-driver-0-1, groupId=spark-kafka-source-71d3a546-2d9e-4f76-a6b9-bc85d9902881--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 4833.\n",
      "23/06/24 15:00:20 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/205 using temp file gs://kafka-spark-data/spark-metadata/offsets/.205.6d021bce-9f83-4973-bbdc-70b32df0eb66.tmp\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"event_name\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"event_id\", StringType()),\n",
    "    StructField(\"event_url\", StringType()),\n",
    "    StructField(\"venue_name\", StringType()),\n",
    "    StructField(\"venue_id\", StringType()),\n",
    "    StructField(\"venue_zipcode\", StringType()),\n",
    "    StructField(\"venues_timezone\", StringType()),\n",
    "    StructField(\"venue_city\", StringType()),\n",
    "    StructField(\"venue_state_full\", StringType()),\n",
    "    StructField(\"venue_state_short\", StringType()),\n",
    "    StructField(\"venue_country_name\", StringType()),\n",
    "    StructField(\"venue_country_short\", StringType()),\n",
    "    StructField(\"venue_address\", StringType()),\n",
    "    StructField(\"venue_longitude\", StringType()),\n",
    "    StructField(\"venue_latitude\", StringType()),\n",
    "    StructField(\"attraction_name\", StringType()),\n",
    "    StructField(\"attraction_type\", StringType()),\n",
    "    StructField(\"attraction_id\", StringType()),\n",
    "    StructField(\"attraction_url\", StringType()),\n",
    "    StructField(\"attraction_segment_id\", StringType()),\n",
    "    StructField(\"attraction_segment_name\", StringType()),\n",
    "    StructField(\"attraction_genre_id\", StringType()),\n",
    "    StructField(\"attraction_genre_name\", StringType()),\n",
    "    StructField(\"attraction_subgenre_id\", StringType()),\n",
    "    StructField(\"attraction_subgenre_name\", StringType()),\n",
    "    StructField(\"event_start_date\", StringType()),\n",
    "    StructField(\"ticket_status\", StringType()),\n",
    "    StructField(\"event_start_time\", StringType()),\n",
    "    StructField(\"currency\", StringType()),\n",
    "    StructField(\"min_price\", StringType()),\n",
    "    StructField(\"max_price\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ID = os.getenv('GCP_PROJECT_ID')\n",
    "BUCKET = os.getenv('GCP_BUCKET')\n",
    "\n",
    "dataset = os.getenv('GCP_dataset')\n",
    "table = os.getenv('GCP_table')\n",
    "\n",
    "gcs_metadata_folder = os.getenv('GCP_metadata_bucket')\n",
    "gcs_data_folder = os.getenv('GCP_data_bucket')\n",
    "\n",
    "print(PROJECT_ID)\n",
    "print(BUCKET)\n",
    "print(dataset)\n",
    "print(gcs_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:43:33 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-60b89fd2-ef43-43f2-8675-88739e516fa4/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:43:33 INFO FileOutputCommitter: Saved output of task 'attempt_202306241543314112936720846474209_0221_m_000000_221' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-60b89fd2-ef43-43f2-8675-88739e516fa4/_temporary/0/task_202306241543314112936720846474209_0221_m_000000\n",
      "23/06/24 15:43:33 INFO SparkHadoopMapRedUtil: attempt_202306241543314112936720846474209_0221_m_000000_221: Committed. Elapsed time: 986 ms.\n",
      "23/06/24 15:43:33 INFO Executor: Finished task 0.0 in stage 221.0 (TID 221). 2536 bytes result sent to driver\n",
      "23/06/24 15:43:33 INFO TaskSetManager: Finished task 0.0 in stage 221.0 (TID 221) in 2083 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:43:33 INFO TaskSchedulerImpl: Removed TaskSet 221.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:43:33 INFO DAGScheduler: ResultStage 221 (start at NativeMethodAccessorImpl.java:0) finished in 2.126 s\n",
      "23/06/24 15:43:33 INFO DAGScheduler: Job 221 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:43:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 221: Stage finished\n",
      "23/06/24 15:43:33 INFO DAGScheduler: Job 221 finished: start at NativeMethodAccessorImpl.java:0, took 2.126878 s\n",
      "23/06/24 15:43:33 INFO FileFormatWriter: Start to commit write Job 9d99a4bb-46e7-4b03-a47d-8917f180ae82.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:43:34 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-60b89fd2-ef43-43f2-8675-88739e516fa4/_temporary/0/task_202306241543314112936720846474209_0221_m_000000/' directory.\n",
      "23/06/24 15:43:34 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-60b89fd2-ef43-43f2-8675-88739e516fa4/' directory.\n",
      "23/06/24 15:43:35 INFO FileFormatWriter: Write Job 9d99a4bb-46e7-4b03-a47d-8917f180ae82 committed. Elapsed time: 2358 ms.\n",
      "23/06/24 15:43:35 INFO FileFormatWriter: Finished processing stats for write job 9d99a4bb-46e7-4b03-a47d-8917f180ae82.\n",
      "23/06/24 15:43:35 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-60b89fd2-ef43-43f2-8675-88739e516fa4/part-00000-6b73874d-63c3-4a63-8c24-35da881084a1-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=c4741b7b-a19a-41ed-b54d-03f37f848223, location=US}\n",
      "23/06/24 15:43:37 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=c4741b7b-a19a-41ed-b54d-03f37f848223, location=US}\n",
      "23/06/24 15:43:38 INFO BlockManagerInfo: Removed broadcast_221_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:43:38 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/424 using temp file gs://kafka-spark-data/spark-metadata/commits/.424.5715a06d-ba8d-46cb-89cd-382760ec74e2.tmp\n",
      "23/06/24 15:43:39 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.424.5715a06d-ba8d-46cb-89cd-382760ec74e2.tmp to gs://kafka-spark-data/spark-metadata/commits/424\n",
      "23/06/24 15:43:39 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:43:28.362Z\",\n",
      "  \"batchId\" : 424,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.23820867079561697,\n",
      "  \"processedRowsPerSecond\" : 0.25929127052722556,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 7787,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 64,\n",
      "    \"triggerExecution\" : 11570,\n",
      "    \"walCommit\" : 2019\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5332\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5335\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.23820867079561697,\n",
      "    \"processedRowsPerSecond\" : 0.25929127052722556\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:43:39 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11570 milliseconds\n",
      "23/06/24 15:43:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:43:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5337.\n",
      "23/06/24 15:43:40 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/425 using temp file gs://kafka-spark-data/spark-metadata/offsets/.425.636a98df-5f7c-4647-9308-b524fb93d927.tmp\n",
      "23/06/24 15:43:41 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.425.636a98df-5f7c-4647-9308-b524fb93d927.tmp to gs://kafka-spark-data/spark-metadata/offsets/425\n",
      "23/06/24 15:43:41 INFO MicroBatchExecution: Committed offsets for batch 425. Metadata OffsetSeqMetadata(0,1687639419938,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:43:41 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:43:41 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:43:41 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:43:41 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:43:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:43:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:43:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:43:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:43:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:43:42 INFO DAGScheduler: Got job 222 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:43:42 INFO DAGScheduler: Final stage: ResultStage 222 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:43:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:43:42 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:43:42 INFO DAGScheduler: Submitting ResultStage 222 (MapPartitionsRDD[1560] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:43:42 INFO MemoryStore: Block broadcast_222 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:43:42 INFO MemoryStore: Block broadcast_222_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:43:42 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:43:42 INFO SparkContext: Created broadcast 222 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:43:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 222 (MapPartitionsRDD[1560] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:43:42 INFO TaskSchedulerImpl: Adding task set 222.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:43:42 INFO TaskSetManager: Starting task 0.0 in stage 222.0 (TID 222) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:43:42 INFO Executor: Running task 0.0 in stage 222.0 (TID 222)\n",
      "23/06/24 15:43:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:43:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:43:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:43:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:43:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:42 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:43:42 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:43:42 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:43:42 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:43:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:43:42 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:43:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:43:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5335 for partition ticketmaster-0\n",
      "23/06/24 15:43:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 222:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:43:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:43:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:43:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5338.\n",
      "23/06/24 15:43:44 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-4de41ce8-d7c6-4b53-9b38-b5f8616bc2b7/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:43:44 INFO FileOutputCommitter: Saved output of task 'attempt_202306241543422219220570663763894_0222_m_000000_222' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-4de41ce8-d7c6-4b53-9b38-b5f8616bc2b7/_temporary/0/task_202306241543422219220570663763894_0222_m_000000\n",
      "23/06/24 15:43:44 INFO SparkHadoopMapRedUtil: attempt_202306241543422219220570663763894_0222_m_000000_222: Committed. Elapsed time: 1040 ms.\n",
      "23/06/24 15:43:44 INFO Executor: Finished task 0.0 in stage 222.0 (TID 222). 2579 bytes result sent to driver\n",
      "23/06/24 15:43:44 INFO TaskSetManager: Finished task 0.0 in stage 222.0 (TID 222) in 2050 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:43:44 INFO TaskSchedulerImpl: Removed TaskSet 222.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:43:44 INFO DAGScheduler: ResultStage 222 (start at NativeMethodAccessorImpl.java:0) finished in 2.065 s\n",
      "23/06/24 15:43:44 INFO DAGScheduler: Job 222 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:43:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 222: Stage finished\n",
      "23/06/24 15:43:44 INFO DAGScheduler: Job 222 finished: start at NativeMethodAccessorImpl.java:0, took 2.066834 s\n",
      "23/06/24 15:43:44 INFO FileFormatWriter: Start to commit write Job 1c28a817-3e7c-4880-a8ec-941c9b222993.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:43:45 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-4de41ce8-d7c6-4b53-9b38-b5f8616bc2b7/_temporary/0/task_202306241543422219220570663763894_0222_m_000000/' directory.\n",
      "23/06/24 15:43:46 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-4de41ce8-d7c6-4b53-9b38-b5f8616bc2b7/' directory.\n",
      "23/06/24 15:43:46 INFO FileFormatWriter: Write Job 1c28a817-3e7c-4880-a8ec-941c9b222993 committed. Elapsed time: 2097 ms.\n",
      "23/06/24 15:43:46 INFO FileFormatWriter: Finished processing stats for write job 1c28a817-3e7c-4880-a8ec-941c9b222993.\n",
      "23/06/24 15:43:46 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-4de41ce8-d7c6-4b53-9b38-b5f8616bc2b7/part-00000-fab90185-989f-4d53-b13e-2d5e95db29a4-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=1adaedc7-c2b5-48b4-8385-2a0e513dc0e9, location=US}\n",
      "23/06/24 15:43:49 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=1adaedc7-c2b5-48b4-8385-2a0e513dc0e9, location=US}\n",
      "23/06/24 15:43:50 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/425 using temp file gs://kafka-spark-data/spark-metadata/commits/.425.44d739c7-5b8c-44ce-9d98-e70650e03a2b.tmp\n",
      "23/06/24 15:43:51 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.425.44d739c7-5b8c-44ce-9d98-e70650e03a2b.tmp to gs://kafka-spark-data/spark-metadata/commits/425\n",
      "23/06/24 15:43:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:43:39.933Z\",\n",
      "  \"batchId\" : 425,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.17284590787313112,\n",
      "  \"processedRowsPerSecond\" : 0.16688918558077437,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 8541,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 11984,\n",
      "    \"walCommit\" : 1943\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5335\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5337\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.17284590787313112,\n",
      "    \"processedRowsPerSecond\" : 0.16688918558077437\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:43:51 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11988 milliseconds\n",
      "23/06/24 15:43:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:43:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5339.\n",
      "23/06/24 15:43:52 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/426 using temp file gs://kafka-spark-data/spark-metadata/offsets/.426.a8cdf99e-0703-4597-8194-2e456905d2b5.tmp\n",
      "23/06/24 15:43:53 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.426.a8cdf99e-0703-4597-8194-2e456905d2b5.tmp to gs://kafka-spark-data/spark-metadata/offsets/426\n",
      "23/06/24 15:43:53 INFO MicroBatchExecution: Committed offsets for batch 426. Metadata OffsetSeqMetadata(0,1687639431934,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:43:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:43:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:43:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:43:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:43:54 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:43:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:43:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:43:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:43:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:43:54 INFO DAGScheduler: Got job 223 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:43:54 INFO DAGScheduler: Final stage: ResultStage 223 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:43:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:43:54 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:43:54 INFO DAGScheduler: Submitting ResultStage 223 (MapPartitionsRDD[1567] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:43:54 INFO MemoryStore: Block broadcast_223 stored as values in memory (estimated size 295.4 KiB, free 433.7 MiB)\n",
      "23/06/24 15:43:54 INFO MemoryStore: Block broadcast_223_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 433.6 MiB)\n",
      "23/06/24 15:43:54 INFO BlockManagerInfo: Added broadcast_223_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.2 MiB)\n",
      "23/06/24 15:43:54 INFO SparkContext: Created broadcast 223 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:43:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 223 (MapPartitionsRDD[1567] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:43:54 INFO TaskSchedulerImpl: Adding task set 223.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:43:54 INFO TaskSetManager: Starting task 0.0 in stage 223.0 (TID 223) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:43:54 INFO Executor: Running task 0.0 in stage 223.0 (TID 223)\n",
      "23/06/24 15:43:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:43:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:43:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:43:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:43:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:43:54 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:43:54 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:43:54 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:43:54 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:43:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:43:54 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:43:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:43:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5337 for partition ticketmaster-0\n",
      "23/06/24 15:43:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n",
      "23/06/24 15:43:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:43:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:43:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5340.\n",
      "23/06/24 15:43:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5338 for partition ticketmaster-0\n",
      "23/06/24 15:43:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 223:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:43:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:43:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:43:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5340.\n",
      "23/06/24 15:43:56 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-f6f5e83b-9ef4-4ab9-884d-6920ec553fd4/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:43:56 INFO FileOutputCommitter: Saved output of task 'attempt_202306241543546812901914039253015_0223_m_000000_223' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-f6f5e83b-9ef4-4ab9-884d-6920ec553fd4/_temporary/0/task_202306241543546812901914039253015_0223_m_000000\n",
      "23/06/24 15:43:56 INFO SparkHadoopMapRedUtil: attempt_202306241543546812901914039253015_0223_m_000000_223: Committed. Elapsed time: 909 ms.\n",
      "23/06/24 15:43:56 INFO Executor: Finished task 0.0 in stage 223.0 (TID 223). 2536 bytes result sent to driver\n",
      "23/06/24 15:43:56 INFO TaskSetManager: Finished task 0.0 in stage 223.0 (TID 223) in 2003 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:43:56 INFO TaskSchedulerImpl: Removed TaskSet 223.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:43:56 INFO DAGScheduler: ResultStage 223 (start at NativeMethodAccessorImpl.java:0) finished in 2.025 s\n",
      "23/06/24 15:43:56 INFO DAGScheduler: Job 223 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:43:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 223: Stage finished\n",
      "23/06/24 15:43:56 INFO DAGScheduler: Job 223 finished: start at NativeMethodAccessorImpl.java:0, took 2.026751 s\n",
      "23/06/24 15:43:56 INFO FileFormatWriter: Start to commit write Job cafbd1c0-3e21-414d-8520-279189665cab.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:43:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-f6f5e83b-9ef4-4ab9-884d-6920ec553fd4/_temporary/0/task_202306241543546812901914039253015_0223_m_000000/' directory.\n",
      "23/06/24 15:43:58 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-f6f5e83b-9ef4-4ab9-884d-6920ec553fd4/' directory.\n",
      "23/06/24 15:43:58 INFO FileFormatWriter: Write Job cafbd1c0-3e21-414d-8520-279189665cab committed. Elapsed time: 2034 ms.\n",
      "23/06/24 15:43:58 INFO FileFormatWriter: Finished processing stats for write job cafbd1c0-3e21-414d-8520-279189665cab.\n",
      "23/06/24 15:43:59 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-f6f5e83b-9ef4-4ab9-884d-6920ec553fd4/part-00000-885b5d33-574f-4c79-9120-375c4073d945-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=391cbbfb-1b0d-4e21-a323-e6ec7997ac70, location=US}\n",
      "23/06/24 15:44:02 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=391cbbfb-1b0d-4e21-a323-e6ec7997ac70, location=US}\n",
      "23/06/24 15:44:03 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/426 using temp file gs://kafka-spark-data/spark-metadata/commits/.426.c030d66d-ee09-427f-920b-9ed550e69ef2.tmp\n",
      "23/06/24 15:44:04 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.426.c030d66d-ee09-427f-920b-9ed550e69ef2.tmp to gs://kafka-spark-data/spark-metadata/commits/426\n",
      "23/06/24 15:44:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:43:51.921Z\",\n",
      "  \"batchId\" : 426,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.16683350016683351,\n",
      "  \"processedRowsPerSecond\" : 0.16122531237404272,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 8794,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 12,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 12405,\n",
      "    \"walCommit\" : 2181\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5337\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5339\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.16683350016683351,\n",
      "    \"processedRowsPerSecond\" : 0.16122531237404272\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:44:04 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12408 milliseconds\n",
      "23/06/24 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5342.\n",
      "23/06/24 15:44:04 INFO BlockManagerInfo: Removed broadcast_223_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:44:04 INFO BlockManagerInfo: Removed broadcast_222_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:44:04 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/427 using temp file gs://kafka-spark-data/spark-metadata/offsets/.427.8c16cd38-0529-4f33-ad2f-63fb46a5e1b6.tmp\n",
      "23/06/24 15:44:05 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.427.8c16cd38-0529-4f33-ad2f-63fb46a5e1b6.tmp to gs://kafka-spark-data/spark-metadata/offsets/427\n",
      "23/06/24 15:44:05 INFO MicroBatchExecution: Committed offsets for batch 427. Metadata OffsetSeqMetadata(0,1687639444336,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:44:06 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:06 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:06 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:06 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:44:07 INFO DAGScheduler: Got job 224 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:44:07 INFO DAGScheduler: Final stage: ResultStage 224 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:44:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:44:07 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:44:07 INFO DAGScheduler: Submitting ResultStage 224 (MapPartitionsRDD[1574] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:44:07 INFO MemoryStore: Block broadcast_224 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:44:07 INFO MemoryStore: Block broadcast_224_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:44:07 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:44:07 INFO SparkContext: Created broadcast 224 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:44:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 224 (MapPartitionsRDD[1574] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:44:07 INFO TaskSchedulerImpl: Adding task set 224.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:44:07 INFO TaskSetManager: Starting task 0.0 in stage 224.0 (TID 224) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:44:07 INFO Executor: Running task 0.0 in stage 224.0 (TID 224)\n",
      "23/06/24 15:44:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:07 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:07 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:07 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:44:07 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:44:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:44:07 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:44:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:44:07 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5340 for partition ticketmaster-0\n",
      "23/06/24 15:44:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 224:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:44:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5342.\n",
      "23/06/24 15:44:09 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cba88029-3b89-47cf-9eed-703c850cd826/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:44:09 INFO FileOutputCommitter: Saved output of task 'attempt_202306241544074985391970721279483_0224_m_000000_224' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cba88029-3b89-47cf-9eed-703c850cd826/_temporary/0/task_202306241544074985391970721279483_0224_m_000000\n",
      "23/06/24 15:44:09 INFO SparkHadoopMapRedUtil: attempt_202306241544074985391970721279483_0224_m_000000_224: Committed. Elapsed time: 993 ms.\n",
      "23/06/24 15:44:09 INFO Executor: Finished task 0.0 in stage 224.0 (TID 224). 2579 bytes result sent to driver\n",
      "23/06/24 15:44:09 INFO TaskSetManager: Finished task 0.0 in stage 224.0 (TID 224) in 2077 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:44:09 INFO TaskSchedulerImpl: Removed TaskSet 224.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:44:09 INFO DAGScheduler: ResultStage 224 (start at NativeMethodAccessorImpl.java:0) finished in 2.115 s\n",
      "23/06/24 15:44:09 INFO DAGScheduler: Job 224 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:44:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 224: Stage finished\n",
      "23/06/24 15:44:09 INFO DAGScheduler: Job 224 finished: start at NativeMethodAccessorImpl.java:0, took 2.115300 s\n",
      "23/06/24 15:44:09 INFO FileFormatWriter: Start to commit write Job 672db8cf-33b2-4e14-9c83-0b7c4b5ed556.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cba88029-3b89-47cf-9eed-703c850cd826/_temporary/0/task_202306241544074985391970721279483_0224_m_000000/' directory.\n",
      "23/06/24 15:44:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cba88029-3b89-47cf-9eed-703c850cd826/' directory.\n",
      "23/06/24 15:44:10 INFO BlockManagerInfo: Removed broadcast_224_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:44:11 INFO FileFormatWriter: Write Job 672db8cf-33b2-4e14-9c83-0b7c4b5ed556 committed. Elapsed time: 2000 ms.\n",
      "23/06/24 15:44:11 INFO FileFormatWriter: Finished processing stats for write job 672db8cf-33b2-4e14-9c83-0b7c4b5ed556.\n",
      "23/06/24 15:44:11 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cba88029-3b89-47cf-9eed-703c850cd826/part-00000-77fc09d6-80e1-41b4-9dcc-4bc585d8cae4-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=c67b22b4-e2fd-468f-818f-7cc15ec32ac3, location=US}\n",
      "23/06/24 15:44:14 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=c67b22b4-e2fd-468f-818f-7cc15ec32ac3, location=US}\n",
      "23/06/24 15:44:15 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/427 using temp file gs://kafka-spark-data/spark-metadata/commits/.427.2f7c5a9a-677d-40c4-b2b6-8733e5d249f6.tmp\n",
      "23/06/24 15:44:16 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.427.2f7c5a9a-677d-40c4-b2b6-8733e5d249f6.tmp to gs://kafka-spark-data/spark-metadata/commits/427\n",
      "23/06/24 15:44:16 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:44:04.329Z\",\n",
      "  \"batchId\" : 427,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.24177949709864605,\n",
      "  \"processedRowsPerSecond\" : 0.23862551702195356,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 8807,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 7,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 12572,\n",
      "    \"walCommit\" : 2126\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5339\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5342\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.24177949709864605,\n",
      "    \"processedRowsPerSecond\" : 0.23862551702195356\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:44:16 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12574 milliseconds\n",
      "23/06/24 15:44:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5344.\n",
      "23/06/24 15:44:17 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/428 using temp file gs://kafka-spark-data/spark-metadata/offsets/.428.846d84ab-04a1-4b8d-b588-c41319a14171.tmp\n",
      "23/06/24 15:44:18 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.428.846d84ab-04a1-4b8d-b588-c41319a14171.tmp to gs://kafka-spark-data/spark-metadata/offsets/428\n",
      "23/06/24 15:44:18 INFO MicroBatchExecution: Committed offsets for batch 428. Metadata OffsetSeqMetadata(0,1687639456924,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:44:18 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:18 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:18 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:18 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:44:19 INFO DAGScheduler: Got job 225 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:44:19 INFO DAGScheduler: Final stage: ResultStage 225 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:44:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:44:19 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:44:19 INFO DAGScheduler: Submitting ResultStage 225 (MapPartitionsRDD[1581] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:44:19 INFO MemoryStore: Block broadcast_225 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:44:19 INFO MemoryStore: Block broadcast_225_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:44:19 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:44:19 INFO SparkContext: Created broadcast 225 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:44:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 225 (MapPartitionsRDD[1581] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:44:19 INFO TaskSchedulerImpl: Adding task set 225.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:44:19 INFO TaskSetManager: Starting task 0.0 in stage 225.0 (TID 225) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:44:19 INFO Executor: Running task 0.0 in stage 225.0 (TID 225)\n",
      "23/06/24 15:44:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:19 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:19 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:19 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:44:19 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:44:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:44:19 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:44:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:44:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5342 for partition ticketmaster-0\n",
      "23/06/24 15:44:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 225:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:44:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5345.\n",
      "23/06/24 15:44:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e6b771fd-70ae-4c1a-9b97-63627a797f51/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:44:21 INFO FileOutputCommitter: Saved output of task 'attempt_202306241544196630773960278918647_0225_m_000000_225' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e6b771fd-70ae-4c1a-9b97-63627a797f51/_temporary/0/task_202306241544196630773960278918647_0225_m_000000\n",
      "23/06/24 15:44:21 INFO SparkHadoopMapRedUtil: attempt_202306241544196630773960278918647_0225_m_000000_225: Committed. Elapsed time: 942 ms.\n",
      "23/06/24 15:44:21 INFO Executor: Finished task 0.0 in stage 225.0 (TID 225). 2579 bytes result sent to driver\n",
      "23/06/24 15:44:21 INFO TaskSetManager: Finished task 0.0 in stage 225.0 (TID 225) in 2046 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:44:21 INFO TaskSchedulerImpl: Removed TaskSet 225.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:44:21 INFO DAGScheduler: ResultStage 225 (start at NativeMethodAccessorImpl.java:0) finished in 2.066 s\n",
      "23/06/24 15:44:21 INFO DAGScheduler: Job 225 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:44:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 225: Stage finished\n",
      "23/06/24 15:44:21 INFO DAGScheduler: Job 225 finished: start at NativeMethodAccessorImpl.java:0, took 2.069243 s\n",
      "23/06/24 15:44:21 INFO FileFormatWriter: Start to commit write Job 27a04ce7-fcde-4b26-ade6-e410e506fd54.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:22 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e6b771fd-70ae-4c1a-9b97-63627a797f51/_temporary/0/task_202306241544196630773960278918647_0225_m_000000/' directory.\n",
      "23/06/24 15:44:22 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e6b771fd-70ae-4c1a-9b97-63627a797f51/' directory.\n",
      "23/06/24 15:44:23 INFO FileFormatWriter: Write Job 27a04ce7-fcde-4b26-ade6-e410e506fd54 committed. Elapsed time: 2006 ms.\n",
      "23/06/24 15:44:23 INFO FileFormatWriter: Finished processing stats for write job 27a04ce7-fcde-4b26-ade6-e410e506fd54.\n",
      "23/06/24 15:44:23 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e6b771fd-70ae-4c1a-9b97-63627a797f51/part-00000-14968a8a-2675-4825-a288-dc631c952eef-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=2527d4da-bb91-4d96-a5ac-597ac676f25c, location=US}\n",
      "23/06/24 15:44:25 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=2527d4da-bb91-4d96-a5ac-597ac676f25c, location=US}\n",
      "23/06/24 15:44:26 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/428 using temp file gs://kafka-spark-data/spark-metadata/commits/.428.42a07a74-fc97-4a3b-81cb-bfd3bb51d773.tmp\n",
      "23/06/24 15:44:28 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.428.42a07a74-fc97-4a3b-81cb-bfd3bb51d773.tmp to gs://kafka-spark-data/spark-metadata/commits/428\n",
      "23/06/24 15:44:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:44:16.903Z\",\n",
      "  \"batchId\" : 428,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.1590583744234134,\n",
      "  \"processedRowsPerSecond\" : 0.17962996227770792,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 7649,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 21,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 11134,\n",
      "    \"walCommit\" : 1919\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5342\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5344\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.1590583744234134,\n",
      "    \"processedRowsPerSecond\" : 0.17962996227770792\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:44:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11135 milliseconds\n",
      "23/06/24 15:44:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5346.\n",
      "23/06/24 15:44:28 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/429 using temp file gs://kafka-spark-data/spark-metadata/offsets/.429.f756ae8b-92fa-48bd-9ee1-3a39e2b8a48b.tmp\n",
      "23/06/24 15:44:29 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.429.f756ae8b-92fa-48bd-9ee1-3a39e2b8a48b.tmp to gs://kafka-spark-data/spark-metadata/offsets/429\n",
      "23/06/24 15:44:29 INFO MicroBatchExecution: Committed offsets for batch 429. Metadata OffsetSeqMetadata(0,1687639468044,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:44:30 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:30 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:30 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:30 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:44:30 INFO DAGScheduler: Got job 226 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:44:30 INFO DAGScheduler: Final stage: ResultStage 226 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:44:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:44:30 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:44:30 INFO DAGScheduler: Submitting ResultStage 226 (MapPartitionsRDD[1588] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:44:30 INFO MemoryStore: Block broadcast_226 stored as values in memory (estimated size 295.4 KiB, free 433.7 MiB)\n",
      "23/06/24 15:44:30 INFO MemoryStore: Block broadcast_226_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 433.6 MiB)\n",
      "23/06/24 15:44:30 INFO BlockManagerInfo: Added broadcast_226_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.2 MiB)\n",
      "23/06/24 15:44:30 INFO SparkContext: Created broadcast 226 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:44:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 226 (MapPartitionsRDD[1588] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:44:30 INFO TaskSchedulerImpl: Adding task set 226.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:44:30 INFO TaskSetManager: Starting task 0.0 in stage 226.0 (TID 226) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:44:30 INFO Executor: Running task 0.0 in stage 226.0 (TID 226)\n",
      "23/06/24 15:44:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:30 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:30 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:44:30 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:44:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:44:30 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:44:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:44:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5345 for partition ticketmaster-0\n",
      "23/06/24 15:44:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:31 INFO BlockManagerInfo: Removed broadcast_225_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 226:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:44:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5347.\n",
      "23/06/24 15:44:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-d8013a6a-587b-4eb1-afb0-3128e0ebc279/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:44:32 INFO FileOutputCommitter: Saved output of task 'attempt_202306241544305750021783249033278_0226_m_000000_226' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-d8013a6a-587b-4eb1-afb0-3128e0ebc279/_temporary/0/task_202306241544305750021783249033278_0226_m_000000\n",
      "23/06/24 15:44:32 INFO SparkHadoopMapRedUtil: attempt_202306241544305750021783249033278_0226_m_000000_226: Committed. Elapsed time: 860 ms.\n",
      "23/06/24 15:44:32 INFO Executor: Finished task 0.0 in stage 226.0 (TID 226). 2579 bytes result sent to driver\n",
      "23/06/24 15:44:32 INFO TaskSetManager: Finished task 0.0 in stage 226.0 (TID 226) in 2000 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:44:32 INFO TaskSchedulerImpl: Removed TaskSet 226.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:44:32 INFO DAGScheduler: ResultStage 226 (start at NativeMethodAccessorImpl.java:0) finished in 2.035 s\n",
      "23/06/24 15:44:32 INFO DAGScheduler: Job 226 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:44:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 226: Stage finished\n",
      "23/06/24 15:44:32 INFO DAGScheduler: Job 226 finished: start at NativeMethodAccessorImpl.java:0, took 2.036623 s\n",
      "23/06/24 15:44:32 INFO FileFormatWriter: Start to commit write Job ba35dbbb-98ec-41a7-b0a6-62a6ca021e4c.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:33 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-d8013a6a-587b-4eb1-afb0-3128e0ebc279/_temporary/0/task_202306241544305750021783249033278_0226_m_000000/' directory.\n",
      "23/06/24 15:44:34 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-d8013a6a-587b-4eb1-afb0-3128e0ebc279/' directory.\n",
      "23/06/24 15:44:35 INFO FileFormatWriter: Write Job ba35dbbb-98ec-41a7-b0a6-62a6ca021e4c committed. Elapsed time: 2196 ms.\n",
      "23/06/24 15:44:35 INFO FileFormatWriter: Finished processing stats for write job ba35dbbb-98ec-41a7-b0a6-62a6ca021e4c.\n",
      "23/06/24 15:44:35 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-d8013a6a-587b-4eb1-afb0-3128e0ebc279/part-00000-0280a6cf-5453-49cf-ab4a-29ab04ad0ae6-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=5b06e7d4-bf36-463e-be05-ef75d9a98bc8, location=US}\n",
      "23/06/24 15:44:38 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=5b06e7d4-bf36-463e-be05-ef75d9a98bc8, location=US}\n",
      "23/06/24 15:44:39 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/429 using temp file gs://kafka-spark-data/spark-metadata/commits/.429.62b80550-f143-4664-938b-967af0ea3508.tmp\n",
      "23/06/24 15:44:40 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.429.62b80550-f143-4664-938b-967af0ea3508.tmp to gs://kafka-spark-data/spark-metadata/commits/429\n",
      "23/06/24 15:44:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:44:28.038Z\",\n",
      "  \"batchId\" : 429,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.1796138302649304,\n",
      "  \"processedRowsPerSecond\" : 0.1610305958132045,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 8951,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 10,\n",
      "    \"triggerExecution\" : 12420,\n",
      "    \"walCommit\" : 2012\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5344\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5346\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.1796138302649304,\n",
      "    \"processedRowsPerSecond\" : 0.1610305958132045\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:44:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12421 milliseconds\n",
      "23/06/24 15:44:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5349.\n",
      "23/06/24 15:44:40 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/430 using temp file gs://kafka-spark-data/spark-metadata/offsets/.430.6e563c89-17de-49b6-afd8-b76df54be558.tmp\n",
      "23/06/24 15:44:41 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.430.6e563c89-17de-49b6-afd8-b76df54be558.tmp to gs://kafka-spark-data/spark-metadata/offsets/430\n",
      "23/06/24 15:44:41 INFO MicroBatchExecution: Committed offsets for batch 430. Metadata OffsetSeqMetadata(0,1687639480465,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:44:42 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:42 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:42 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:42 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:44:43 INFO DAGScheduler: Got job 227 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:44:43 INFO DAGScheduler: Final stage: ResultStage 227 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:44:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:44:43 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:44:43 INFO DAGScheduler: Submitting ResultStage 227 (MapPartitionsRDD[1595] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:44:43 INFO MemoryStore: Block broadcast_227 stored as values in memory (estimated size 295.4 KiB, free 433.7 MiB)\n",
      "23/06/24 15:44:43 INFO MemoryStore: Block broadcast_227_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 433.6 MiB)\n",
      "23/06/24 15:44:43 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.2 MiB)\n",
      "23/06/24 15:44:43 INFO SparkContext: Created broadcast 227 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:44:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 227 (MapPartitionsRDD[1595] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:44:43 INFO TaskSchedulerImpl: Adding task set 227.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:44:43 INFO TaskSetManager: Starting task 0.0 in stage 227.0 (TID 227) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:44:43 INFO Executor: Running task 0.0 in stage 227.0 (TID 227)\n",
      "23/06/24 15:44:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:43 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:43 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:43 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:44:43 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:44:43 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:44:43 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:44:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:44:43 INFO BlockManagerInfo: Removed broadcast_226_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:44:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5347 for partition ticketmaster-0\n",
      "23/06/24 15:44:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 227:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:44:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5350.\n",
      "23/06/24 15:44:45 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-c52e0172-b544-46ae-9710-354754fd83c2/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:44:45 INFO FileOutputCommitter: Saved output of task 'attempt_202306241544438373234766100047334_0227_m_000000_227' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-c52e0172-b544-46ae-9710-354754fd83c2/_temporary/0/task_202306241544438373234766100047334_0227_m_000000\n",
      "23/06/24 15:44:45 INFO SparkHadoopMapRedUtil: attempt_202306241544438373234766100047334_0227_m_000000_227: Committed. Elapsed time: 1000 ms.\n",
      "23/06/24 15:44:45 INFO Executor: Finished task 0.0 in stage 227.0 (TID 227). 2536 bytes result sent to driver\n",
      "23/06/24 15:44:45 INFO TaskSetManager: Finished task 0.0 in stage 227.0 (TID 227) in 2093 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:44:45 INFO TaskSchedulerImpl: Removed TaskSet 227.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:44:45 INFO DAGScheduler: ResultStage 227 (start at NativeMethodAccessorImpl.java:0) finished in 2.131 s\n",
      "23/06/24 15:44:45 INFO DAGScheduler: Job 227 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:44:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 227: Stage finished\n",
      "23/06/24 15:44:45 INFO DAGScheduler: Job 227 finished: start at NativeMethodAccessorImpl.java:0, took 2.131453 s\n",
      "23/06/24 15:44:45 INFO FileFormatWriter: Start to commit write Job 0731c535-2be5-437b-b7eb-7b20954cd166.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:46 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-c52e0172-b544-46ae-9710-354754fd83c2/_temporary/0/task_202306241544438373234766100047334_0227_m_000000/' directory.\n",
      "23/06/24 15:44:46 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-c52e0172-b544-46ae-9710-354754fd83c2/' directory.\n",
      "23/06/24 15:44:47 INFO FileFormatWriter: Write Job 0731c535-2be5-437b-b7eb-7b20954cd166 committed. Elapsed time: 2064 ms.\n",
      "23/06/24 15:44:47 INFO FileFormatWriter: Finished processing stats for write job 0731c535-2be5-437b-b7eb-7b20954cd166.\n",
      "23/06/24 15:44:47 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-c52e0172-b544-46ae-9710-354754fd83c2/part-00000-7faae07c-eaaa-41e1-bfef-f0e1cec8a927-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=465e8155-a10c-4024-8fba-32098de0af77, location=US}\n",
      "23/06/24 15:44:50 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=465e8155-a10c-4024-8fba-32098de0af77, location=US}\n",
      "23/06/24 15:44:51 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/430 using temp file gs://kafka-spark-data/spark-metadata/commits/.430.1cc0953c-f09a-4e83-89e6-6fa0e809bc5f.tmp\n",
      "23/06/24 15:44:52 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.430.1cc0953c-f09a-4e83-89e6-6fa0e809bc5f.tmp to gs://kafka-spark-data/spark-metadata/commits/430\n",
      "23/06/24 15:44:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:44:40.459Z\",\n",
      "  \"batchId\" : 430,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.2415264471459625,\n",
      "  \"processedRowsPerSecond\" : 0.24127392633102784,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 8835,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 10,\n",
      "    \"triggerExecution\" : 12434,\n",
      "    \"walCommit\" : 2038\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5346\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5349\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.2415264471459625,\n",
      "    \"processedRowsPerSecond\" : 0.24127392633102784\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:44:52 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12435 milliseconds\n",
      "23/06/24 15:44:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5351.\n",
      "23/06/24 15:44:53 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/431 using temp file gs://kafka-spark-data/spark-metadata/offsets/.431.6bb8b978-5d9c-41b3-8e6b-72a462aa325f.tmp\n",
      "23/06/24 15:44:53 INFO BlockManagerInfo: Removed broadcast_227_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:44:54 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.431.6bb8b978-5d9c-41b3-8e6b-72a462aa325f.tmp to gs://kafka-spark-data/spark-metadata/offsets/431\n",
      "23/06/24 15:44:54 INFO MicroBatchExecution: Committed offsets for batch 431. Metadata OffsetSeqMetadata(0,1687639492901,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:44:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:54 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:44:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:44:55 INFO DAGScheduler: Got job 228 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:44:55 INFO DAGScheduler: Final stage: ResultStage 228 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:44:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:44:55 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:44:55 INFO DAGScheduler: Submitting ResultStage 228 (MapPartitionsRDD[1602] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:44:55 INFO MemoryStore: Block broadcast_228 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:44:55 INFO MemoryStore: Block broadcast_228_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:44:55 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:44:55 INFO SparkContext: Created broadcast 228 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:44:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 228 (MapPartitionsRDD[1602] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:44:55 INFO TaskSchedulerImpl: Adding task set 228.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:44:55 INFO TaskSetManager: Starting task 0.0 in stage 228.0 (TID 228) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:44:55 INFO Executor: Running task 0.0 in stage 228.0 (TID 228)\n",
      "23/06/24 15:44:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:44:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:44:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:44:55 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:55 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:44:55 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:44:55 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:44:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:44:55 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:44:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:44:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5350 for partition ticketmaster-0\n",
      "23/06/24 15:44:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 228:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:44:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:44:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5352.\n",
      "23/06/24 15:44:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-316a656c-9935-469c-9ca7-5ecf8e2ab1c6/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_20230624154455860535605805953295_0228_m_000000_228' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-316a656c-9935-469c-9ca7-5ecf8e2ab1c6/_temporary/0/task_20230624154455860535605805953295_0228_m_000000\n",
      "23/06/24 15:44:57 INFO SparkHadoopMapRedUtil: attempt_20230624154455860535605805953295_0228_m_000000_228: Committed. Elapsed time: 854 ms.\n",
      "23/06/24 15:44:57 INFO Executor: Finished task 0.0 in stage 228.0 (TID 228). 2536 bytes result sent to driver\n",
      "23/06/24 15:44:57 INFO TaskSetManager: Finished task 0.0 in stage 228.0 (TID 228) in 2052 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:44:57 INFO TaskSchedulerImpl: Removed TaskSet 228.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:44:57 INFO DAGScheduler: ResultStage 228 (start at NativeMethodAccessorImpl.java:0) finished in 2.070 s\n",
      "23/06/24 15:44:57 INFO DAGScheduler: Job 228 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:44:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 228: Stage finished\n",
      "23/06/24 15:44:57 INFO DAGScheduler: Job 228 finished: start at NativeMethodAccessorImpl.java:0, took 2.070752 s\n",
      "23/06/24 15:44:57 INFO FileFormatWriter: Start to commit write Job 9b0fac8d-e493-425e-9803-09afba129b47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:44:58 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-316a656c-9935-469c-9ca7-5ecf8e2ab1c6/_temporary/0/task_20230624154455860535605805953295_0228_m_000000/' directory.\n",
      "23/06/24 15:44:59 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-316a656c-9935-469c-9ca7-5ecf8e2ab1c6/' directory.\n",
      "23/06/24 15:44:59 INFO FileFormatWriter: Write Job 9b0fac8d-e493-425e-9803-09afba129b47 committed. Elapsed time: 2262 ms.\n",
      "23/06/24 15:44:59 INFO FileFormatWriter: Finished processing stats for write job 9b0fac8d-e493-425e-9803-09afba129b47.\n",
      "23/06/24 15:45:00 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-316a656c-9935-469c-9ca7-5ecf8e2ab1c6/part-00000-bb6a5a87-592b-426d-8975-d1a2c5825719-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=d6704c07-3bb2-41cf-8b28-da0315ad49dc, location=US}\n",
      "23/06/24 15:45:03 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=d6704c07-3bb2-41cf-8b28-da0315ad49dc, location=US}\n",
      "23/06/24 15:45:04 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/431 using temp file gs://kafka-spark-data/spark-metadata/commits/.431.9daa3fab-853a-4d9e-8e38-d122ee16c0cc.tmp\n",
      "23/06/24 15:45:05 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.431.9daa3fab-853a-4d9e-8e38-d122ee16c0cc.tmp to gs://kafka-spark-data/spark-metadata/commits/431\n",
      "23/06/24 15:45:05 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:44:52.894Z\",\n",
      "  \"batchId\" : 431,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.16083634901487737,\n",
      "  \"processedRowsPerSecond\" : 0.15802781289506954,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 9096,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 14,\n",
      "    \"triggerExecution\" : 12656,\n",
      "    \"walCommit\" : 2038\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5349\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5351\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.16083634901487737,\n",
      "    \"processedRowsPerSecond\" : 0.15802781289506954\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:45:05 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12658 milliseconds\n",
      "23/06/24 15:45:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5354.\n",
      "23/06/24 15:45:05 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/432 using temp file gs://kafka-spark-data/spark-metadata/offsets/.432.4bc99f71-4abe-4905-bb64-4d37ce02e561.tmp\n",
      "23/06/24 15:45:06 INFO BlockManagerInfo: Removed broadcast_228_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:45:07 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.432.4bc99f71-4abe-4905-bb64-4d37ce02e561.tmp to gs://kafka-spark-data/spark-metadata/offsets/432\n",
      "23/06/24 15:45:07 INFO MicroBatchExecution: Committed offsets for batch 432. Metadata OffsetSeqMetadata(0,1687639505568,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:45:07 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:07 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:07 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:07 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:45:08 INFO DAGScheduler: Got job 229 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:45:08 INFO DAGScheduler: Final stage: ResultStage 229 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:45:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:45:08 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:45:08 INFO DAGScheduler: Submitting ResultStage 229 (MapPartitionsRDD[1609] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:45:08 INFO MemoryStore: Block broadcast_229 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:45:08 INFO MemoryStore: Block broadcast_229_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:45:08 INFO BlockManagerInfo: Added broadcast_229_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:45:08 INFO SparkContext: Created broadcast 229 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:45:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 229 (MapPartitionsRDD[1609] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:45:08 INFO TaskSchedulerImpl: Adding task set 229.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:45:08 INFO TaskSetManager: Starting task 0.0 in stage 229.0 (TID 229) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:45:08 INFO Executor: Running task 0.0 in stage 229.0 (TID 229)\n",
      "23/06/24 15:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:08 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:08 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:08 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:45:08 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:45:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:45:08 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:45:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:45:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5352 for partition ticketmaster-0\n",
      "23/06/24 15:45:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 229:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:45:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5355.\n",
      "23/06/24 15:45:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-b67f0e02-4b5d-4abd-acd5-075c6d204635/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:45:10 INFO FileOutputCommitter: Saved output of task 'attempt_202306241545087765221744616854592_0229_m_000000_229' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-b67f0e02-4b5d-4abd-acd5-075c6d204635/_temporary/0/task_202306241545087765221744616854592_0229_m_000000\n",
      "23/06/24 15:45:10 INFO SparkHadoopMapRedUtil: attempt_202306241545087765221744616854592_0229_m_000000_229: Committed. Elapsed time: 1017 ms.\n",
      "23/06/24 15:45:10 INFO Executor: Finished task 0.0 in stage 229.0 (TID 229). 2536 bytes result sent to driver\n",
      "23/06/24 15:45:10 INFO TaskSetManager: Finished task 0.0 in stage 229.0 (TID 229) in 2198 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:45:10 INFO TaskSchedulerImpl: Removed TaskSet 229.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:45:10 INFO DAGScheduler: ResultStage 229 (start at NativeMethodAccessorImpl.java:0) finished in 2.240 s\n",
      "23/06/24 15:45:10 INFO DAGScheduler: Job 229 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:45:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 229: Stage finished\n",
      "23/06/24 15:45:10 INFO DAGScheduler: Job 229 finished: start at NativeMethodAccessorImpl.java:0, took 2.240968 s\n",
      "23/06/24 15:45:10 INFO FileFormatWriter: Start to commit write Job b8f8ef7c-d330-4c89-879c-68130ef698b6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-b67f0e02-4b5d-4abd-acd5-075c6d204635/_temporary/0/task_202306241545087765221744616854592_0229_m_000000/' directory.\n",
      "23/06/24 15:45:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-b67f0e02-4b5d-4abd-acd5-075c6d204635/' directory.\n",
      "23/06/24 15:45:12 INFO FileFormatWriter: Write Job b8f8ef7c-d330-4c89-879c-68130ef698b6 committed. Elapsed time: 1888 ms.\n",
      "23/06/24 15:45:12 INFO FileFormatWriter: Finished processing stats for write job b8f8ef7c-d330-4c89-879c-68130ef698b6.\n",
      "23/06/24 15:45:12 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-b67f0e02-4b5d-4abd-acd5-075c6d204635/part-00000-66016500-ba2e-494a-8cb6-05801d554d97-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=f33f1cf4-c187-472d-bce7-8dec76a00bbd, location=US}\n",
      "23/06/24 15:45:14 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=f33f1cf4-c187-472d-bce7-8dec76a00bbd, location=US}\n",
      "23/06/24 15:45:15 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/432 using temp file gs://kafka-spark-data/spark-metadata/commits/.432.eddcf275-5715-4d9a-a68f-a913eab3a8c9.tmp\n",
      "23/06/24 15:45:16 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.432.eddcf275-5715-4d9a-a68f-a913eab3a8c9.tmp to gs://kafka-spark-data/spark-metadata/commits/432\n",
      "23/06/24 15:45:16 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:45:05.552Z\",\n",
      "  \"batchId\" : 432,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.2370042660767894,\n",
      "  \"processedRowsPerSecond\" : 0.2691307078137615,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 7615,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 16,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 11147,\n",
      "    \"walCommit\" : 2030\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5351\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5354\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.2370042660767894,\n",
      "    \"processedRowsPerSecond\" : 0.2691307078137615\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:45:16 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11148 milliseconds\n",
      "23/06/24 15:45:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5356.\n",
      "23/06/24 15:45:16 INFO BlockManagerInfo: Removed broadcast_229_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:45:17 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/433 using temp file gs://kafka-spark-data/spark-metadata/offsets/.433.c1ec4a94-c161-4630-8af6-4387f70ad434.tmp\n",
      "23/06/24 15:45:18 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.433.c1ec4a94-c161-4630-8af6-4387f70ad434.tmp to gs://kafka-spark-data/spark-metadata/offsets/433\n",
      "23/06/24 15:45:18 INFO MicroBatchExecution: Committed offsets for batch 433. Metadata OffsetSeqMetadata(0,1687639516706,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:45:18 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:18 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:18 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:18 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:45:19 INFO DAGScheduler: Got job 230 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:45:19 INFO DAGScheduler: Final stage: ResultStage 230 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:45:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:45:19 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:45:19 INFO DAGScheduler: Submitting ResultStage 230 (MapPartitionsRDD[1616] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:45:19 INFO MemoryStore: Block broadcast_230 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:45:19 INFO MemoryStore: Block broadcast_230_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:45:19 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:45:19 INFO SparkContext: Created broadcast 230 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:45:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 230 (MapPartitionsRDD[1616] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:45:19 INFO TaskSchedulerImpl: Adding task set 230.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:45:19 INFO TaskSetManager: Starting task 0.0 in stage 230.0 (TID 230) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:45:19 INFO Executor: Running task 0.0 in stage 230.0 (TID 230)\n",
      "23/06/24 15:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:19 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:19 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:45:19 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:45:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:45:19 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:45:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:45:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5355 for partition ticketmaster-0\n",
      "23/06/24 15:45:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 230:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:45:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5357.\n",
      "23/06/24 15:45:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-edaae3d0-5c48-4892-a3d7-e1dc9d7651a7/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:45:21 INFO FileOutputCommitter: Saved output of task 'attempt_202306241545198428101437658095633_0230_m_000000_230' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-edaae3d0-5c48-4892-a3d7-e1dc9d7651a7/_temporary/0/task_202306241545198428101437658095633_0230_m_000000\n",
      "23/06/24 15:45:21 INFO SparkHadoopMapRedUtil: attempt_202306241545198428101437658095633_0230_m_000000_230: Committed. Elapsed time: 931 ms.\n",
      "23/06/24 15:45:21 INFO Executor: Finished task 0.0 in stage 230.0 (TID 230). 2536 bytes result sent to driver\n",
      "23/06/24 15:45:21 INFO TaskSetManager: Finished task 0.0 in stage 230.0 (TID 230) in 2004 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:45:21 INFO TaskSchedulerImpl: Removed TaskSet 230.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:45:21 INFO DAGScheduler: ResultStage 230 (start at NativeMethodAccessorImpl.java:0) finished in 2.038 s\n",
      "23/06/24 15:45:21 INFO DAGScheduler: Job 230 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:45:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 230: Stage finished\n",
      "23/06/24 15:45:21 INFO DAGScheduler: Job 230 finished: start at NativeMethodAccessorImpl.java:0, took 2.039318 s\n",
      "23/06/24 15:45:21 INFO FileFormatWriter: Start to commit write Job 49f880d1-a26e-496e-90f4-d6a816306c90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:22 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-edaae3d0-5c48-4892-a3d7-e1dc9d7651a7/_temporary/0/task_202306241545198428101437658095633_0230_m_000000/' directory.\n",
      "23/06/24 15:45:22 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-edaae3d0-5c48-4892-a3d7-e1dc9d7651a7/' directory.\n",
      "23/06/24 15:45:23 INFO FileFormatWriter: Write Job 49f880d1-a26e-496e-90f4-d6a816306c90 committed. Elapsed time: 1796 ms.\n",
      "23/06/24 15:45:23 INFO FileFormatWriter: Finished processing stats for write job 49f880d1-a26e-496e-90f4-d6a816306c90.\n",
      "23/06/24 15:45:23 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-edaae3d0-5c48-4892-a3d7-e1dc9d7651a7/part-00000-e180a29e-c490-4179-abbf-2963a4032709-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=1a95e9dd-7d8e-469b-b940-bf6c9a572490, location=US}\n",
      "23/06/24 15:45:26 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=1a95e9dd-7d8e-469b-b940-bf6c9a572490, location=US}\n",
      "23/06/24 15:45:27 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/433 using temp file gs://kafka-spark-data/spark-metadata/commits/.433.645b8fc2-3a8c-4409-b6db-aa1a41221b91.tmp\n",
      "23/06/24 15:45:28 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.433.645b8fc2-3a8c-4409-b6db-aa1a41221b91.tmp to gs://kafka-spark-data/spark-metadata/commits/433\n",
      "23/06/24 15:45:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:45:16.700Z\",\n",
      "  \"batchId\" : 433,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.1794043774668102,\n",
      "  \"processedRowsPerSecond\" : 0.1640554507423509,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 8736,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 12191,\n",
      "    \"walCommit\" : 2055\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5354\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5356\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.1794043774668102,\n",
      "    \"processedRowsPerSecond\" : 0.1640554507423509\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:45:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12195 milliseconds\n",
      "23/06/24 15:45:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5359.\n",
      "23/06/24 15:45:28 INFO BlockManagerInfo: Removed broadcast_230_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:45:29 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/434 using temp file gs://kafka-spark-data/spark-metadata/offsets/.434.9805592e-e34f-4de1-93d1-d1df1c00dbe7.tmp\n",
      "23/06/24 15:45:30 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.434.9805592e-e34f-4de1-93d1-d1df1c00dbe7.tmp to gs://kafka-spark-data/spark-metadata/offsets/434\n",
      "23/06/24 15:45:30 INFO MicroBatchExecution: Committed offsets for batch 434. Metadata OffsetSeqMetadata(0,1687639528900,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:45:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:45:31 INFO DAGScheduler: Got job 231 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:45:31 INFO DAGScheduler: Final stage: ResultStage 231 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:45:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:45:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:45:31 INFO DAGScheduler: Submitting ResultStage 231 (MapPartitionsRDD[1623] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:45:31 INFO MemoryStore: Block broadcast_231 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:45:31 INFO MemoryStore: Block broadcast_231_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:45:31 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:45:31 INFO SparkContext: Created broadcast 231 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:45:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 231 (MapPartitionsRDD[1623] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:45:31 INFO TaskSchedulerImpl: Adding task set 231.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:45:31 INFO TaskSetManager: Starting task 0.0 in stage 231.0 (TID 231) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:45:31 INFO Executor: Running task 0.0 in stage 231.0 (TID 231)\n",
      "23/06/24 15:45:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:31 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:31 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:31 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:45:31 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:45:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:45:31 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:45:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:45:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5357 for partition ticketmaster-0\n",
      "23/06/24 15:45:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 231:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:45:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5359.\n",
      "23/06/24 15:45:33 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ad3a8f3a-dcad-484d-9906-d70ec73d05e8/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:45:33 INFO FileOutputCommitter: Saved output of task 'attempt_202306241545316374939825352523789_0231_m_000000_231' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ad3a8f3a-dcad-484d-9906-d70ec73d05e8/_temporary/0/task_202306241545316374939825352523789_0231_m_000000\n",
      "23/06/24 15:45:33 INFO SparkHadoopMapRedUtil: attempt_202306241545316374939825352523789_0231_m_000000_231: Committed. Elapsed time: 879 ms.\n",
      "23/06/24 15:45:33 INFO Executor: Finished task 0.0 in stage 231.0 (TID 231). 2536 bytes result sent to driver\n",
      "23/06/24 15:45:33 INFO TaskSetManager: Finished task 0.0 in stage 231.0 (TID 231) in 1974 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:45:33 INFO TaskSchedulerImpl: Removed TaskSet 231.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:45:33 INFO DAGScheduler: ResultStage 231 (start at NativeMethodAccessorImpl.java:0) finished in 2.006 s\n",
      "23/06/24 15:45:33 INFO DAGScheduler: Job 231 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:45:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 231: Stage finished\n",
      "23/06/24 15:45:33 INFO DAGScheduler: Job 231 finished: start at NativeMethodAccessorImpl.java:0, took 2.007800 s\n",
      "23/06/24 15:45:33 INFO FileFormatWriter: Start to commit write Job b19dbb10-0350-43e1-9cbb-34190c60fc34.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:34 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ad3a8f3a-dcad-484d-9906-d70ec73d05e8/_temporary/0/task_202306241545316374939825352523789_0231_m_000000/' directory.\n",
      "23/06/24 15:45:35 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ad3a8f3a-dcad-484d-9906-d70ec73d05e8/' directory.\n",
      "23/06/24 15:45:35 INFO FileFormatWriter: Write Job b19dbb10-0350-43e1-9cbb-34190c60fc34 committed. Elapsed time: 2011 ms.\n",
      "23/06/24 15:45:35 INFO FileFormatWriter: Finished processing stats for write job b19dbb10-0350-43e1-9cbb-34190c60fc34.\n",
      "23/06/24 15:45:36 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ad3a8f3a-dcad-484d-9906-d70ec73d05e8/part-00000-09ed1d3f-9a70-4f88-8d34-888ab4b051d2-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=e52e2a06-968d-4bfa-8f82-211aefcf959c, location=US}\n",
      "23/06/24 15:45:38 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=e52e2a06-968d-4bfa-8f82-211aefcf959c, location=US}\n",
      "23/06/24 15:45:38 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/434 using temp file gs://kafka-spark-data/spark-metadata/commits/.434.df1da17d-7c51-46d1-b326-61702a0fb548.tmp\n",
      "23/06/24 15:45:38 INFO BlockManagerInfo: Removed broadcast_231_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:45:40 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.434.df1da17d-7c51-46d1-b326-61702a0fb548.tmp to gs://kafka-spark-data/spark-metadata/commits/434\n",
      "23/06/24 15:45:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:45:28.895Z\",\n",
      "  \"batchId\" : 434,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.24600246002460024,\n",
      "  \"processedRowsPerSecond\" : 0.2665482007996446,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 7544,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 11255,\n",
      "    \"walCommit\" : 2186\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5356\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5359\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.24600246002460024,\n",
      "    \"processedRowsPerSecond\" : 0.2665482007996446\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:45:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11256 milliseconds\n",
      "23/06/24 15:45:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5361.\n",
      "23/06/24 15:45:40 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/435 using temp file gs://kafka-spark-data/spark-metadata/offsets/.435.4b87737a-6f19-4269-93d9-10ea7dc18fa4.tmp\n",
      "23/06/24 15:45:41 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.435.4b87737a-6f19-4269-93d9-10ea7dc18fa4.tmp to gs://kafka-spark-data/spark-metadata/offsets/435\n",
      "23/06/24 15:45:41 INFO MicroBatchExecution: Committed offsets for batch 435. Metadata OffsetSeqMetadata(0,1687639540164,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:45:42 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:42 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:42 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:42 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:45:42 INFO DAGScheduler: Got job 232 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:45:42 INFO DAGScheduler: Final stage: ResultStage 232 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:45:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:45:42 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:45:42 INFO DAGScheduler: Submitting ResultStage 232 (MapPartitionsRDD[1630] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:45:42 INFO MemoryStore: Block broadcast_232 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:45:42 INFO MemoryStore: Block broadcast_232_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:45:42 INFO BlockManagerInfo: Added broadcast_232_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:45:42 INFO SparkContext: Created broadcast 232 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:45:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 232 (MapPartitionsRDD[1630] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:45:42 INFO TaskSchedulerImpl: Adding task set 232.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:45:42 INFO TaskSetManager: Starting task 0.0 in stage 232.0 (TID 232) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:45:42 INFO Executor: Running task 0.0 in stage 232.0 (TID 232)\n",
      "23/06/24 15:45:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:42 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:42 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:42 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:45:42 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:45:42 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:45:42 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:45:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:45:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5359 for partition ticketmaster-0\n",
      "23/06/24 15:45:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:45:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5362.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 232:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:44 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cc3fb3c9-b4cd-461b-bee2-1ebfa4070989/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:45:44 INFO FileOutputCommitter: Saved output of task 'attempt_202306241545426242691699373386998_0232_m_000000_232' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cc3fb3c9-b4cd-461b-bee2-1ebfa4070989/_temporary/0/task_202306241545426242691699373386998_0232_m_000000\n",
      "23/06/24 15:45:44 INFO SparkHadoopMapRedUtil: attempt_202306241545426242691699373386998_0232_m_000000_232: Committed. Elapsed time: 1045 ms.\n",
      "23/06/24 15:45:44 INFO Executor: Finished task 0.0 in stage 232.0 (TID 232). 2536 bytes result sent to driver\n",
      "23/06/24 15:45:44 INFO TaskSetManager: Finished task 0.0 in stage 232.0 (TID 232) in 1794 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:45:44 INFO TaskSchedulerImpl: Removed TaskSet 232.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:45:44 INFO DAGScheduler: ResultStage 232 (start at NativeMethodAccessorImpl.java:0) finished in 1.819 s\n",
      "23/06/24 15:45:44 INFO DAGScheduler: Job 232 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:45:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 232: Stage finished\n",
      "23/06/24 15:45:44 INFO DAGScheduler: Job 232 finished: start at NativeMethodAccessorImpl.java:0, took 1.819954 s\n",
      "23/06/24 15:45:44 INFO FileFormatWriter: Start to commit write Job ddcaf957-0eca-41d6-986d-1a141e6995ec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:45 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cc3fb3c9-b4cd-461b-bee2-1ebfa4070989/_temporary/0/task_202306241545426242691699373386998_0232_m_000000/' directory.\n",
      "23/06/24 15:45:46 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cc3fb3c9-b4cd-461b-bee2-1ebfa4070989/' directory.\n",
      "23/06/24 15:45:46 INFO FileFormatWriter: Write Job ddcaf957-0eca-41d6-986d-1a141e6995ec committed. Elapsed time: 2090 ms.\n",
      "23/06/24 15:45:46 INFO FileFormatWriter: Finished processing stats for write job ddcaf957-0eca-41d6-986d-1a141e6995ec.\n",
      "23/06/24 15:45:47 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-cc3fb3c9-b4cd-461b-bee2-1ebfa4070989/part-00000-5b9e51fd-7f8c-4126-bde1-70bbe84720da-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=aea7a57f-62a7-4ce9-b4b1-91f47b28c79d, location=US}\n",
      "23/06/24 15:45:51 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=aea7a57f-62a7-4ce9-b4b1-91f47b28c79d, location=US}\n",
      "23/06/24 15:45:51 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/435 using temp file gs://kafka-spark-data/spark-metadata/commits/.435.3045f6bf-6fd5-4c49-b361-ef56c187fcbf.tmp\n",
      "23/06/24 15:45:51 INFO BlockManagerInfo: Removed broadcast_232_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:45:52 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.435.3045f6bf-6fd5-4c49-b361-ef56c187fcbf.tmp to gs://kafka-spark-data/spark-metadata/commits/435\n",
      "23/06/24 15:45:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:45:40.151Z\",\n",
      "  \"batchId\" : 435,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.17768301350390903,\n",
      "  \"processedRowsPerSecond\" : 0.1557026080186843,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 9288,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 13,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 12845,\n",
      "    \"walCommit\" : 2086\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5359\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5361\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.17768301350390903,\n",
      "    \"processedRowsPerSecond\" : 0.1557026080186843\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:45:53 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12848 milliseconds\n",
      "23/06/24 15:45:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5363.\n",
      "23/06/24 15:45:53 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/436 using temp file gs://kafka-spark-data/spark-metadata/offsets/.436.bc5a08d2-a113-47c7-aeac-a47e21818298.tmp\n",
      "23/06/24 15:45:54 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.436.bc5a08d2-a113-47c7-aeac-a47e21818298.tmp to gs://kafka-spark-data/spark-metadata/offsets/436\n",
      "23/06/24 15:45:54 INFO MicroBatchExecution: Committed offsets for batch 436. Metadata OffsetSeqMetadata(0,1687639553006,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:45:55 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:55 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:55 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:55 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:45:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:45:55 INFO DAGScheduler: Got job 233 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:45:55 INFO DAGScheduler: Final stage: ResultStage 233 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:45:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:45:55 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:45:55 INFO DAGScheduler: Submitting ResultStage 233 (MapPartitionsRDD[1637] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:45:55 INFO MemoryStore: Block broadcast_233 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:45:55 INFO MemoryStore: Block broadcast_233_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:45:55 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:45:55 INFO SparkContext: Created broadcast 233 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:45:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 233 (MapPartitionsRDD[1637] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:45:55 INFO TaskSchedulerImpl: Adding task set 233.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:45:55 INFO TaskSetManager: Starting task 0.0 in stage 233.0 (TID 233) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:45:55 INFO Executor: Running task 0.0 in stage 233.0 (TID 233)\n",
      "23/06/24 15:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:45:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:45:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:45:55 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:55 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:45:55 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:45:55 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:45:55 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:45:55 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:45:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:45:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5361 for partition ticketmaster-0\n",
      "23/06/24 15:45:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:45:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5364.\n",
      "23/06/24 15:45:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5362 for partition ticketmaster-0\n",
      "23/06/24 15:45:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 233:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:45:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:45:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5364.\n",
      "23/06/24 15:45:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ecffd656-4fee-4a14-8fc6-d6d0d67397ef/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:45:57 INFO FileOutputCommitter: Saved output of task 'attempt_202306241545555701057066379465333_0233_m_000000_233' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ecffd656-4fee-4a14-8fc6-d6d0d67397ef/_temporary/0/task_202306241545555701057066379465333_0233_m_000000\n",
      "23/06/24 15:45:57 INFO SparkHadoopMapRedUtil: attempt_202306241545555701057066379465333_0233_m_000000_233: Committed. Elapsed time: 932 ms.\n",
      "23/06/24 15:45:57 INFO Executor: Finished task 0.0 in stage 233.0 (TID 233). 2536 bytes result sent to driver\n",
      "23/06/24 15:45:57 INFO TaskSetManager: Finished task 0.0 in stage 233.0 (TID 233) in 2007 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:45:57 INFO TaskSchedulerImpl: Removed TaskSet 233.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:45:57 INFO DAGScheduler: ResultStage 233 (start at NativeMethodAccessorImpl.java:0) finished in 2.036 s\n",
      "23/06/24 15:45:57 INFO DAGScheduler: Job 233 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:45:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 233: Stage finished\n",
      "23/06/24 15:45:57 INFO DAGScheduler: Job 233 finished: start at NativeMethodAccessorImpl.java:0, took 2.036475 s\n",
      "23/06/24 15:45:57 INFO FileFormatWriter: Start to commit write Job 96034c67-62e9-470d-943c-03cf0651790e.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:45:58 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ecffd656-4fee-4a14-8fc6-d6d0d67397ef/_temporary/0/task_202306241545555701057066379465333_0233_m_000000/' directory.\n",
      "23/06/24 15:45:59 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ecffd656-4fee-4a14-8fc6-d6d0d67397ef/' directory.\n",
      "23/06/24 15:45:59 INFO FileFormatWriter: Write Job 96034c67-62e9-470d-943c-03cf0651790e committed. Elapsed time: 1893 ms.\n",
      "23/06/24 15:45:59 INFO FileFormatWriter: Finished processing stats for write job 96034c67-62e9-470d-943c-03cf0651790e.\n",
      "23/06/24 15:46:00 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-ecffd656-4fee-4a14-8fc6-d6d0d67397ef/part-00000-87e787ee-1f35-4c01-b042-472e5926a1e9-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=905cadc5-5044-4ae9-ad22-ca4822f4d833, location=US}\n",
      "23/06/24 15:46:02 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=905cadc5-5044-4ae9-ad22-ca4822f4d833, location=US}\n",
      "23/06/24 15:46:03 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/436 using temp file gs://kafka-spark-data/spark-metadata/commits/.436.a1c91915-66fa-4997-b9ba-3096df75a611.tmp\n",
      "23/06/24 15:46:03 INFO BlockManagerInfo: Removed broadcast_233_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:46:04 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.436.a1c91915-66fa-4997-b9ba-3096df75a611.tmp to gs://kafka-spark-data/spark-metadata/commits/436\n",
      "23/06/24 15:46:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:45:53.000Z\",\n",
      "  \"batchId\" : 436,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.1556541365086777,\n",
      "  \"processedRowsPerSecond\" : 0.18001800180018002,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 7609,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 11110,\n",
      "    \"walCommit\" : 2043\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5361\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5363\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.1556541365086777,\n",
      "    \"processedRowsPerSecond\" : 0.18001800180018002\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:46:04 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11112 milliseconds\n",
      "23/06/24 15:46:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:46:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5366.\n",
      "23/06/24 15:46:04 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/437 using temp file gs://kafka-spark-data/spark-metadata/offsets/.437.fa480073-06b0-4c87-b196-4f6bab08eb3c.tmp\n",
      "23/06/24 15:46:05 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.437.fa480073-06b0-4c87-b196-4f6bab08eb3c.tmp to gs://kafka-spark-data/spark-metadata/offsets/437\n",
      "23/06/24 15:46:05 INFO MicroBatchExecution: Committed offsets for batch 437. Metadata OffsetSeqMetadata(0,1687639564122,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:46:06 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:06 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:06 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:06 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:46:07 INFO DAGScheduler: Got job 234 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:46:07 INFO DAGScheduler: Final stage: ResultStage 234 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:46:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:46:07 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:46:07 INFO DAGScheduler: Submitting ResultStage 234 (MapPartitionsRDD[1644] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:46:07 INFO MemoryStore: Block broadcast_234 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:46:07 INFO MemoryStore: Block broadcast_234_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:46:07 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:46:07 INFO SparkContext: Created broadcast 234 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:46:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 234 (MapPartitionsRDD[1644] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:46:07 INFO TaskSchedulerImpl: Adding task set 234.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:46:07 INFO TaskSetManager: Starting task 0.0 in stage 234.0 (TID 234) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:46:07 INFO Executor: Running task 0.0 in stage 234.0 (TID 234)\n",
      "23/06/24 15:46:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:07 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:46:07 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:46:07 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:46:07 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:46:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:46:07 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:46:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:46:07 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5364 for partition ticketmaster-0\n",
      "23/06/24 15:46:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 234:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:46:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:46:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:46:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5366.\n",
      "23/06/24 15:46:09 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7870f21e-d406-493f-8e5c-52aafb726bd7/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:46:09 INFO FileOutputCommitter: Saved output of task 'attempt_20230624154607668688817708867172_0234_m_000000_234' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7870f21e-d406-493f-8e5c-52aafb726bd7/_temporary/0/task_20230624154607668688817708867172_0234_m_000000\n",
      "23/06/24 15:46:09 INFO SparkHadoopMapRedUtil: attempt_20230624154607668688817708867172_0234_m_000000_234: Committed. Elapsed time: 811 ms.\n",
      "23/06/24 15:46:09 INFO Executor: Finished task 0.0 in stage 234.0 (TID 234). 2536 bytes result sent to driver\n",
      "23/06/24 15:46:09 INFO TaskSetManager: Finished task 0.0 in stage 234.0 (TID 234) in 2053 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:46:09 INFO TaskSchedulerImpl: Removed TaskSet 234.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:46:09 INFO DAGScheduler: ResultStage 234 (start at NativeMethodAccessorImpl.java:0) finished in 2.091 s\n",
      "23/06/24 15:46:09 INFO DAGScheduler: Job 234 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:46:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 234: Stage finished\n",
      "23/06/24 15:46:09 INFO DAGScheduler: Job 234 finished: start at NativeMethodAccessorImpl.java:0, took 2.092374 s\n",
      "23/06/24 15:46:09 INFO FileFormatWriter: Start to commit write Job b51c467e-fc0e-4632-9a81-5543f6b4664d.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:46:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7870f21e-d406-493f-8e5c-52aafb726bd7/_temporary/0/task_20230624154607668688817708867172_0234_m_000000/' directory.\n",
      "23/06/24 15:46:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7870f21e-d406-493f-8e5c-52aafb726bd7/' directory.\n",
      "23/06/24 15:46:11 INFO FileFormatWriter: Write Job b51c467e-fc0e-4632-9a81-5543f6b4664d committed. Elapsed time: 2196 ms.\n",
      "23/06/24 15:46:11 INFO FileFormatWriter: Finished processing stats for write job b51c467e-fc0e-4632-9a81-5543f6b4664d.\n",
      "23/06/24 15:46:11 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7870f21e-d406-493f-8e5c-52aafb726bd7/part-00000-fff636d1-200b-4fb0-878c-421b55264dc9-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=6d78f67b-0375-4639-9e90-2b4d9a2e9b90, location=US}\n",
      "23/06/24 15:46:13 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=6d78f67b-0375-4639-9e90-2b4d9a2e9b90, location=US}\n",
      "23/06/24 15:46:14 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/437 using temp file gs://kafka-spark-data/spark-metadata/commits/.437.92e845bf-16fa-4557-83db-518f40a2e9eb.tmp\n",
      "23/06/24 15:46:14 INFO BlockManagerInfo: Removed broadcast_234_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:46:15 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.437.92e845bf-16fa-4557-83db-518f40a2e9eb.tmp to gs://kafka-spark-data/spark-metadata/commits/437\n",
      "23/06/24 15:46:15 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:46:04.113Z\",\n",
      "  \"batchId\" : 437,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.2699541078016737,\n",
      "  \"processedRowsPerSecond\" : 0.26661926768574473,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 7463,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 9,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 11252,\n",
      "    \"walCommit\" : 2191\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5363\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5366\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.2699541078016737,\n",
      "    \"processedRowsPerSecond\" : 0.26661926768574473\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:46:15 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11257 milliseconds\n",
      "23/06/24 15:46:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:46:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5368.\n",
      "23/06/24 15:46:15 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/438 using temp file gs://kafka-spark-data/spark-metadata/offsets/.438.b42d70e9-1f14-4d91-b217-fcc213b6bef4.tmp\n",
      "23/06/24 15:46:16 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.438.b42d70e9-1f14-4d91-b217-fcc213b6bef4.tmp to gs://kafka-spark-data/spark-metadata/offsets/438\n",
      "23/06/24 15:46:16 INFO MicroBatchExecution: Committed offsets for batch 438. Metadata OffsetSeqMetadata(0,1687639575385,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:46:17 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:17 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:17 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:17 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:46:18 INFO DAGScheduler: Got job 235 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:46:18 INFO DAGScheduler: Final stage: ResultStage 235 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:46:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:46:18 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:46:18 INFO DAGScheduler: Submitting ResultStage 235 (MapPartitionsRDD[1651] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:46:18 INFO MemoryStore: Block broadcast_235 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:46:18 INFO MemoryStore: Block broadcast_235_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:46:18 INFO BlockManagerInfo: Added broadcast_235_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:46:18 INFO SparkContext: Created broadcast 235 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:46:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 235 (MapPartitionsRDD[1651] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:46:18 INFO TaskSchedulerImpl: Adding task set 235.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:46:18 INFO TaskSetManager: Starting task 0.0 in stage 235.0 (TID 235) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:46:18 INFO Executor: Running task 0.0 in stage 235.0 (TID 235)\n",
      "23/06/24 15:46:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:18 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:46:18 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:46:18 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:46:18 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:46:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:46:18 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:46:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:46:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5366 for partition ticketmaster-0\n",
      "23/06/24 15:46:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 235:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:46:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:46:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:46:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5369.\n",
      "23/06/24 15:46:20 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-baa70b38-90fe-4c8d-a67f-b228d4140518/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:46:20 INFO FileOutputCommitter: Saved output of task 'attempt_202306241546183320456958041564764_0235_m_000000_235' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-baa70b38-90fe-4c8d-a67f-b228d4140518/_temporary/0/task_202306241546183320456958041564764_0235_m_000000\n",
      "23/06/24 15:46:20 INFO SparkHadoopMapRedUtil: attempt_202306241546183320456958041564764_0235_m_000000_235: Committed. Elapsed time: 902 ms.\n",
      "23/06/24 15:46:20 INFO Executor: Finished task 0.0 in stage 235.0 (TID 235). 2536 bytes result sent to driver\n",
      "23/06/24 15:46:20 INFO TaskSetManager: Finished task 0.0 in stage 235.0 (TID 235) in 2015 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:46:20 INFO TaskSchedulerImpl: Removed TaskSet 235.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:46:20 INFO DAGScheduler: ResultStage 235 (start at NativeMethodAccessorImpl.java:0) finished in 2.033 s\n",
      "23/06/24 15:46:20 INFO DAGScheduler: Job 235 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:46:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 235: Stage finished\n",
      "23/06/24 15:46:20 INFO DAGScheduler: Job 235 finished: start at NativeMethodAccessorImpl.java:0, took 2.034772 s\n",
      "23/06/24 15:46:20 INFO FileFormatWriter: Start to commit write Job 6c002eed-f225-4712-ad59-6c5fc7092d91.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:46:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-baa70b38-90fe-4c8d-a67f-b228d4140518/_temporary/0/task_202306241546183320456958041564764_0235_m_000000/' directory.\n",
      "23/06/24 15:46:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-baa70b38-90fe-4c8d-a67f-b228d4140518/' directory.\n",
      "23/06/24 15:46:22 INFO FileFormatWriter: Write Job 6c002eed-f225-4712-ad59-6c5fc7092d91 committed. Elapsed time: 2094 ms.\n",
      "23/06/24 15:46:22 INFO FileFormatWriter: Finished processing stats for write job 6c002eed-f225-4712-ad59-6c5fc7092d91.\n",
      "23/06/24 15:46:22 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-baa70b38-90fe-4c8d-a67f-b228d4140518/part-00000-11e03362-96cb-4a68-be0e-a885e85a6e44-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=a5f03a96-b549-4fcd-bbdf-383916df0352, location=US}\n",
      "23/06/24 15:46:27 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=a5f03a96-b549-4fcd-bbdf-383916df0352, location=US}\n",
      "23/06/24 15:46:28 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/438 using temp file gs://kafka-spark-data/spark-metadata/commits/.438.5bceaeed-c235-4265-aad4-0e91f0aeb3fa.tmp\n",
      "23/06/24 15:46:28 INFO BlockManagerInfo: Removed broadcast_235_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:46:29 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.438.5bceaeed-c235-4265-aad4-0e91f0aeb3fa.tmp to gs://kafka-spark-data/spark-metadata/commits/438\n",
      "23/06/24 15:46:29 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:46:15.370Z\",\n",
      "  \"batchId\" : 438,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.17766722927955939,\n",
      "  \"processedRowsPerSecond\" : 0.14033118158854896,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 10450,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 14,\n",
      "    \"queryPlanning\" : 14,\n",
      "    \"triggerExecution\" : 14252,\n",
      "    \"walCommit\" : 2150\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5366\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5368\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.17766722927955939,\n",
      "    \"processedRowsPerSecond\" : 0.14033118158854896\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:46:29 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 14256 milliseconds\n",
      "23/06/24 15:46:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:46:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5371.\n",
      "23/06/24 15:46:29 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/439 using temp file gs://kafka-spark-data/spark-metadata/offsets/.439.3998ac07-8b4d-42f8-96cd-fe0baf55c5e5.tmp\n",
      "23/06/24 15:46:31 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.439.3998ac07-8b4d-42f8-96cd-fe0baf55c5e5.tmp to gs://kafka-spark-data/spark-metadata/offsets/439\n",
      "23/06/24 15:46:31 INFO MicroBatchExecution: Committed offsets for batch 439. Metadata OffsetSeqMetadata(0,1687639589648,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:46:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:31 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:46:32 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:46:32 INFO DAGScheduler: Got job 236 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:46:32 INFO DAGScheduler: Final stage: ResultStage 236 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:46:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:46:32 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:46:32 INFO DAGScheduler: Submitting ResultStage 236 (MapPartitionsRDD[1658] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:46:32 INFO MemoryStore: Block broadcast_236 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:46:32 INFO MemoryStore: Block broadcast_236_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:46:32 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:46:32 INFO SparkContext: Created broadcast 236 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:46:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 236 (MapPartitionsRDD[1658] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:46:32 INFO TaskSchedulerImpl: Adding task set 236.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:46:32 INFO TaskSetManager: Starting task 0.0 in stage 236.0 (TID 236) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:46:32 INFO Executor: Running task 0.0 in stage 236.0 (TID 236)\n",
      "23/06/24 15:46:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:46:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:46:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:46:32 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:46:32 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:46:32 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:46:32 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:46:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:46:32 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:46:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:46:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5369 for partition ticketmaster-0\n",
      "23/06/24 15:46:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 236:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:46:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:46:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:46:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5371.\n",
      "23/06/24 15:46:34 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-1c514615-e52f-41e8-8338-cc7f4addb6d2/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:46:34 INFO FileOutputCommitter: Saved output of task 'attempt_202306241546326010417713276490322_0236_m_000000_236' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-1c514615-e52f-41e8-8338-cc7f4addb6d2/_temporary/0/task_202306241546326010417713276490322_0236_m_000000\n",
      "23/06/24 15:46:34 INFO SparkHadoopMapRedUtil: attempt_202306241546326010417713276490322_0236_m_000000_236: Committed. Elapsed time: 827 ms.\n",
      "23/06/24 15:46:34 INFO Executor: Finished task 0.0 in stage 236.0 (TID 236). 2579 bytes result sent to driver\n",
      "23/06/24 15:46:34 INFO TaskSetManager: Finished task 0.0 in stage 236.0 (TID 236) in 1916 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:46:34 INFO TaskSchedulerImpl: Removed TaskSet 236.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:46:34 INFO DAGScheduler: ResultStage 236 (start at NativeMethodAccessorImpl.java:0) finished in 1.971 s\n",
      "23/06/24 15:46:34 INFO DAGScheduler: Job 236 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:46:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 236: Stage finished\n",
      "23/06/24 15:46:34 INFO DAGScheduler: Job 236 finished: start at NativeMethodAccessorImpl.java:0, took 1.972063 s\n",
      "23/06/24 15:46:34 INFO FileFormatWriter: Start to commit write Job eccba82d-bff6-4a4a-a56e-a838e7a82e2d.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:46:35 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-1c514615-e52f-41e8-8338-cc7f4addb6d2/_temporary/0/task_202306241546326010417713276490322_0236_m_000000/' directory.\n",
      "23/06/24 15:46:36 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-1c514615-e52f-41e8-8338-cc7f4addb6d2/' directory.\n",
      "23/06/24 15:46:36 INFO FileFormatWriter: Write Job eccba82d-bff6-4a4a-a56e-a838e7a82e2d committed. Elapsed time: 2271 ms.\n",
      "23/06/24 15:46:36 INFO FileFormatWriter: Finished processing stats for write job eccba82d-bff6-4a4a-a56e-a838e7a82e2d.\n",
      "23/06/24 15:46:36 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-1c514615-e52f-41e8-8338-cc7f4addb6d2/part-00000-1601c98e-d07b-4113-bd8e-336ca1f8e8cf-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=5728ff19-e984-4a5c-aef4-b5829217e6c3, location=US}\n",
      "23/06/24 15:47:19 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=5728ff19-e984-4a5c-aef4-b5829217e6c3, location=US}\n",
      "23/06/24 15:47:20 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/439 using temp file gs://kafka-spark-data/spark-metadata/commits/.439.3a8a40d1-771a-47ea-bbfd-59ec56008bbb.tmp\n",
      "23/06/24 15:47:21 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.439.3a8a40d1-771a-47ea-bbfd-59ec56008bbb.tmp to gs://kafka-spark-data/spark-metadata/commits/439\n",
      "23/06/24 15:47:21 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:46:29.626Z\",\n",
      "  \"batchId\" : 439,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.21043771043771042,\n",
      "  \"processedRowsPerSecond\" : 0.05804728919159475,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 48458,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 21,\n",
      "    \"queryPlanning\" : 20,\n",
      "    \"triggerExecution\" : 51682,\n",
      "    \"walCommit\" : 2068\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5368\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5371\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.21043771043771042,\n",
      "    \"processedRowsPerSecond\" : 0.05804728919159475\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:47:21 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 51684 milliseconds\n",
      "23/06/24 15:47:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:47:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:47:21 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/440 using temp file gs://kafka-spark-data/spark-metadata/offsets/.440.1884fac4-771c-4189-9507-ef583ee152cc.tmp\n",
      "23/06/24 15:47:21 INFO BlockManagerInfo: Removed broadcast_236_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:47:22 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.440.1884fac4-771c-4189-9507-ef583ee152cc.tmp to gs://kafka-spark-data/spark-metadata/offsets/440\n",
      "23/06/24 15:47:22 INFO MicroBatchExecution: Committed offsets for batch 440. Metadata OffsetSeqMetadata(0,1687639641315,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:47:22 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:47:22 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:47:22 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:47:22 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:47:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:47:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:47:23 INFO DAGScheduler: Got job 237 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:47:23 INFO DAGScheduler: Final stage: ResultStage 237 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:47:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:47:23 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:47:23 INFO DAGScheduler: Submitting ResultStage 237 (MapPartitionsRDD[1665] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:47:23 INFO MemoryStore: Block broadcast_237 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:47:23 INFO MemoryStore: Block broadcast_237_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:47:23 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:47:23 INFO SparkContext: Created broadcast 237 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:47:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 237 (MapPartitionsRDD[1665] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:47:23 INFO TaskSchedulerImpl: Adding task set 237.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:47:23 INFO TaskSetManager: Starting task 0.0 in stage 237.0 (TID 237) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:47:23 INFO Executor: Running task 0.0 in stage 237.0 (TID 237)\n",
      "23/06/24 15:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:47:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:47:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:47:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:47:23 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:47:23 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:47:23 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:47:23 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:47:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:47:23 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:47:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:47:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5371 for partition ticketmaster-0\n",
      "23/06/24 15:47:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 237:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:47:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:47:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:47:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-5, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:47:24 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-5019e852-d7ac-43e2-b496-f2f319ccf148/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:47:24 INFO FileOutputCommitter: Saved output of task 'attempt_202306241547231561773205856950615_0237_m_000000_237' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-5019e852-d7ac-43e2-b496-f2f319ccf148/_temporary/0/task_202306241547231561773205856950615_0237_m_000000\n",
      "23/06/24 15:47:24 INFO SparkHadoopMapRedUtil: attempt_202306241547231561773205856950615_0237_m_000000_237: Committed. Elapsed time: 583 ms.\n",
      "23/06/24 15:47:24 INFO Executor: Finished task 0.0 in stage 237.0 (TID 237). 2536 bytes result sent to driver\n",
      "23/06/24 15:47:24 INFO TaskSetManager: Finished task 0.0 in stage 237.0 (TID 237) in 1518 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:47:24 INFO TaskSchedulerImpl: Removed TaskSet 237.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:47:24 INFO DAGScheduler: ResultStage 237 (start at NativeMethodAccessorImpl.java:0) finished in 1.537 s\n",
      "23/06/24 15:47:24 INFO DAGScheduler: Job 237 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:47:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 237: Stage finished\n",
      "23/06/24 15:47:24 INFO DAGScheduler: Job 237 finished: start at NativeMethodAccessorImpl.java:0, took 1.539030 s\n",
      "23/06/24 15:47:24 INFO FileFormatWriter: Start to commit write Job a0025a09-45ff-4d04-b862-613980e4099f.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:47:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-5019e852-d7ac-43e2-b496-f2f319ccf148/_temporary/0/task_202306241547231561773205856950615_0237_m_000000/' directory.\n",
      "23/06/24 15:47:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-5019e852-d7ac-43e2-b496-f2f319ccf148/' directory.\n",
      "23/06/24 15:47:27 INFO FileFormatWriter: Write Job a0025a09-45ff-4d04-b862-613980e4099f committed. Elapsed time: 2075 ms.\n",
      "23/06/24 15:47:27 INFO FileFormatWriter: Finished processing stats for write job a0025a09-45ff-4d04-b862-613980e4099f.\n",
      "23/06/24 15:47:27 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-5019e852-d7ac-43e2-b496-f2f319ccf148/part-00000-c0fcd024-891c-4205-bef6-340505f1516b-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=3d8eba9a-4906-4083-9f52-38922083c70b, location=US}\n",
      "23/06/24 15:47:29 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=3d8eba9a-4906-4083-9f52-38922083c70b, location=US}\n",
      "23/06/24 15:47:30 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/440 using temp file gs://kafka-spark-data/spark-metadata/commits/.440.b9577ac4-5247-435a-b5ef-8d0679e27ace.tmp\n",
      "23/06/24 15:47:30 INFO BlockManagerInfo: Removed broadcast_237_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:47:31 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.440.b9577ac4-5247-435a-b5ef-8d0679e27ace.tmp to gs://kafka-spark-data/spark-metadata/commits/440\n",
      "23/06/24 15:47:31 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:47:21.310Z\",\n",
      "  \"batchId\" : 440,\n",
      "  \"numInputRows\" : 9,\n",
      "  \"inputRowsPerSecond\" : 0.17413512885999535,\n",
      "  \"processedRowsPerSecond\" : 0.9059794644654722,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 7325,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 18,\n",
      "    \"triggerExecution\" : 9934,\n",
      "    \"walCommit\" : 1491\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5371\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 9,\n",
      "    \"inputRowsPerSecond\" : 0.17413512885999535,\n",
      "    \"processedRowsPerSecond\" : 0.9059794644654722\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:47:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:47:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:47:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:47:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:47:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:47:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:47:50 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:47:50.000Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 6,\n",
      "    \"triggerExecution\" : 6\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:48:00 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:48:00.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 17,\n",
      "    \"triggerExecution\" : 17\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:48:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:48:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:48:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:48:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:48:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:48:20.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 14,\n",
      "    \"triggerExecution\" : 16\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:48:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:48:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:48:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:48:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:48:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:48:40.003Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 4,\n",
      "    \"triggerExecution\" : 4\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:48:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:48:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:48:50 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:48:50.002Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 11,\n",
      "    \"triggerExecution\" : 11\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:49:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:49:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:49:00 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:49:00.010Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 5,\n",
      "    \"triggerExecution\" : 6\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:49:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:49:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:49:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:49:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:49:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:49:20.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 2,\n",
      "    \"triggerExecution\" : 3\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:49:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:49:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:49:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:49:30.004Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 10,\n",
      "    \"triggerExecution\" : 12\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:49:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:49:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:49:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:49:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:49:50 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:49:50.002Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 4,\n",
      "    \"triggerExecution\" : 4\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:50:00 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:50:00.002Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 5,\n",
      "    \"triggerExecution\" : 6\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:50:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:50:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:50:10 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:50:10.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 13,\n",
      "    \"triggerExecution\" : 14\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:50:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:50:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:50:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:50:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:50:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:50:30.001Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 8,\n",
      "    \"triggerExecution\" : 10\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:50:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:50:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:50:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:50:40.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 9,\n",
      "    \"triggerExecution\" : 10\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:50:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:50:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:51:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:51:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:51:00 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:51:00.004Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 22,\n",
      "    \"triggerExecution\" : 23\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:51:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:51:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:51:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:51:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:51:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:51:20.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 7,\n",
      "    \"triggerExecution\" : 7\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:51:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:51:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:51:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:51:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:51:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:51:40.006Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 12,\n",
      "    \"triggerExecution\" : 12\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:51:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:51:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:52:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:52:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:52:00 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:52:00.004Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 19,\n",
      "    \"triggerExecution\" : 20\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:52:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:52:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:52:10 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:52:10.006Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 26,\n",
      "    \"triggerExecution\" : 26\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:52:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:52:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:52:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:52:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:52:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:52:30.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 3,\n",
      "    \"triggerExecution\" : 3\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:52:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:52:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:52:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:52:40.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 6,\n",
      "    \"triggerExecution\" : 6\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:52:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:52:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:53:00 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:53:00.010Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 22,\n",
      "    \"triggerExecution\" : 22\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:53:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5380.\n",
      "23/06/24 15:53:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5382.\n",
      "23/06/24 15:53:20 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/441 using temp file gs://kafka-spark-data/spark-metadata/offsets/.441.1df4e0ae-3940-4be5-89d2-8b537abd98e3.tmp\n",
      "23/06/24 15:53:22 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.441.1df4e0ae-3940-4be5-89d2-8b537abd98e3.tmp to gs://kafka-spark-data/spark-metadata/offsets/441\n",
      "23/06/24 15:53:22 INFO MicroBatchExecution: Committed offsets for batch 441. Metadata OffsetSeqMetadata(0,1687640000011,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:53:22 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:22 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:22 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:22 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:53:23 INFO DAGScheduler: Got job 238 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:53:23 INFO DAGScheduler: Final stage: ResultStage 238 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:53:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:53:23 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:53:23 INFO DAGScheduler: Submitting ResultStage 238 (MapPartitionsRDD[1672] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:53:23 INFO MemoryStore: Block broadcast_238 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:53:23 INFO MemoryStore: Block broadcast_238_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:53:23 INFO BlockManagerInfo: Added broadcast_238_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:53:23 INFO SparkContext: Created broadcast 238 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:53:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 238 (MapPartitionsRDD[1672] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:53:23 INFO TaskSchedulerImpl: Adding task set 238.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:53:23 INFO TaskSetManager: Starting task 0.0 in stage 238.0 (TID 238) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:53:23 INFO Executor: Running task 0.0 in stage 238.0 (TID 238)\n",
      "23/06/24 15:53:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:23 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:53:23 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:53:23 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:53:23 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:53:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:53:23 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:53:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:53:24 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.offset.reset = none\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 500\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "23/06/24 15:53:24 INFO AppInfoParser: Kafka version: 2.6.0\n",
      "23/06/24 15:53:24 INFO AppInfoParser: Kafka commitId: 62abe01bee039651\n",
      "23/06/24 15:53:24 INFO AppInfoParser: Kafka startTimeMs: 1687640004101\n",
      "23/06/24 15:53:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Subscribed to partition(s): ticketmaster-0\n",
      "23/06/24 15:53:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5380 for partition ticketmaster-0\n",
      "23/06/24 15:53:24 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Cluster ID: kJYfpPSDQYKOEac6knVYNg\n",
      "23/06/24 15:53:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 238:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:53:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:53:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5383.\n",
      "23/06/24 15:53:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6531f00d-af48-4c37-b72e-a5864b285562/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:53:26 INFO FileOutputCommitter: Saved output of task 'attempt_202306241553238060599031040295961_0238_m_000000_238' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6531f00d-af48-4c37-b72e-a5864b285562/_temporary/0/task_202306241553238060599031040295961_0238_m_000000\n",
      "23/06/24 15:53:26 INFO SparkHadoopMapRedUtil: attempt_202306241553238060599031040295961_0238_m_000000_238: Committed. Elapsed time: 1021 ms.\n",
      "23/06/24 15:53:26 INFO Executor: Finished task 0.0 in stage 238.0 (TID 238). 2579 bytes result sent to driver\n",
      "23/06/24 15:53:26 INFO TaskSetManager: Finished task 0.0 in stage 238.0 (TID 238) in 2314 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:53:26 INFO TaskSchedulerImpl: Removed TaskSet 238.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:53:26 INFO DAGScheduler: ResultStage 238 (start at NativeMethodAccessorImpl.java:0) finished in 2.357 s\n",
      "23/06/24 15:53:26 INFO DAGScheduler: Job 238 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:53:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 238: Stage finished\n",
      "23/06/24 15:53:26 INFO DAGScheduler: Job 238 finished: start at NativeMethodAccessorImpl.java:0, took 2.358273 s\n",
      "23/06/24 15:53:26 INFO FileFormatWriter: Start to commit write Job 7744cf02-1910-4f7b-9d04-e20135216392.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:53:27 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6531f00d-af48-4c37-b72e-a5864b285562/_temporary/0/task_202306241553238060599031040295961_0238_m_000000/' directory.\n",
      "23/06/24 15:53:27 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6531f00d-af48-4c37-b72e-a5864b285562/' directory.\n",
      "23/06/24 15:53:28 INFO BlockManagerInfo: Removed broadcast_238_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:53:28 INFO FileFormatWriter: Write Job 7744cf02-1910-4f7b-9d04-e20135216392 committed. Elapsed time: 2314 ms.\n",
      "23/06/24 15:53:28 INFO FileFormatWriter: Finished processing stats for write job 7744cf02-1910-4f7b-9d04-e20135216392.\n",
      "23/06/24 15:53:29 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6531f00d-af48-4c37-b72e-a5864b285562/part-00000-93bdf06c-9901-4b0e-a91d-38107c2c66f3-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=b580f8d8-ea66-4ef2-a205-29aac19512b4, location=US}\n",
      "23/06/24 15:53:31 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=b580f8d8-ea66-4ef2-a205-29aac19512b4, location=US}\n",
      "23/06/24 15:53:32 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/441 using temp file gs://kafka-spark-data/spark-metadata/commits/.441.32b71809-46ec-4a3e-a9a9-1859e3324937.tmp\n",
      "23/06/24 15:53:33 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.441.32b71809-46ec-4a3e-a9a9-1859e3324937.tmp to gs://kafka-spark-data/spark-metadata/commits/441\n",
      "23/06/24 15:53:33 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:53:20.005Z\",\n",
      "  \"batchId\" : 441,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.19992003198720512,\n",
      "  \"processedRowsPerSecond\" : 0.14630577907827358,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 9045,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 13670,\n",
      "    \"walCommit\" : 2799\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5380\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5382\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.19992003198720512,\n",
      "    \"processedRowsPerSecond\" : 0.14630577907827358\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:53:33 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 13671 milliseconds\n",
      "23/06/24 15:53:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5385.\n",
      "23/06/24 15:53:34 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/442 using temp file gs://kafka-spark-data/spark-metadata/offsets/.442.2065b053-8bc5-45fb-aace-e286881ee9ae.tmp\n",
      "23/06/24 15:53:35 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.442.2065b053-8bc5-45fb-aace-e286881ee9ae.tmp to gs://kafka-spark-data/spark-metadata/offsets/442\n",
      "23/06/24 15:53:35 INFO MicroBatchExecution: Committed offsets for batch 442. Metadata OffsetSeqMetadata(0,1687640013680,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:53:36 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:36 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:36 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:36 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:53:37 INFO DAGScheduler: Got job 239 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:53:37 INFO DAGScheduler: Final stage: ResultStage 239 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:53:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:53:37 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:53:37 INFO DAGScheduler: Submitting ResultStage 239 (MapPartitionsRDD[1679] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:53:37 INFO MemoryStore: Block broadcast_239 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:53:37 INFO MemoryStore: Block broadcast_239_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:53:37 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:53:37 INFO SparkContext: Created broadcast 239 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:53:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 239 (MapPartitionsRDD[1679] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:53:37 INFO TaskSchedulerImpl: Adding task set 239.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:53:37 INFO TaskSetManager: Starting task 0.0 in stage 239.0 (TID 239) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:53:37 INFO Executor: Running task 0.0 in stage 239.0 (TID 239)\n",
      "23/06/24 15:53:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:37 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:53:37 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:53:37 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:53:37 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:53:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:53:37 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:53:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:53:37 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5383 for partition ticketmaster-0\n",
      "23/06/24 15:53:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 239:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:53:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:53:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5385.\n",
      "23/06/24 15:53:39 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6ff3057f-163d-45f4-91bb-276d29a149a4/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:53:39 INFO FileOutputCommitter: Saved output of task 'attempt_202306241553367973894061413901518_0239_m_000000_239' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6ff3057f-163d-45f4-91bb-276d29a149a4/_temporary/0/task_202306241553367973894061413901518_0239_m_000000\n",
      "23/06/24 15:53:39 INFO SparkHadoopMapRedUtil: attempt_202306241553367973894061413901518_0239_m_000000_239: Committed. Elapsed time: 1068 ms.\n",
      "23/06/24 15:53:39 INFO Executor: Finished task 0.0 in stage 239.0 (TID 239). 2579 bytes result sent to driver\n",
      "23/06/24 15:53:39 INFO TaskSetManager: Finished task 0.0 in stage 239.0 (TID 239) in 2463 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:53:39 INFO TaskSchedulerImpl: Removed TaskSet 239.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:53:39 INFO DAGScheduler: ResultStage 239 (start at NativeMethodAccessorImpl.java:0) finished in 2.494 s\n",
      "23/06/24 15:53:39 INFO DAGScheduler: Job 239 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:53:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 239: Stage finished\n",
      "23/06/24 15:53:39 INFO DAGScheduler: Job 239 finished: start at NativeMethodAccessorImpl.java:0, took 2.495643 s\n",
      "23/06/24 15:53:39 INFO FileFormatWriter: Start to commit write Job bacf3884-8e1f-4c40-acdb-e19fb2a076d2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:53:40 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6ff3057f-163d-45f4-91bb-276d29a149a4/_temporary/0/task_202306241553367973894061413901518_0239_m_000000/' directory.\n",
      "23/06/24 15:53:41 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6ff3057f-163d-45f4-91bb-276d29a149a4/' directory.\n",
      "23/06/24 15:53:41 INFO BlockManagerInfo: Removed broadcast_239_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:53:41 INFO FileFormatWriter: Write Job bacf3884-8e1f-4c40-acdb-e19fb2a076d2 committed. Elapsed time: 2426 ms.\n",
      "23/06/24 15:53:41 INFO FileFormatWriter: Finished processing stats for write job bacf3884-8e1f-4c40-acdb-e19fb2a076d2.\n",
      "23/06/24 15:53:42 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-6ff3057f-163d-45f4-91bb-276d29a149a4/part-00000-d6d372d4-07f4-41a8-98d8-99545bd034ce-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=36bffe0b-0af5-4d1f-bdcf-fb75b18d8130, location=US}\n",
      "23/06/24 15:53:45 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=36bffe0b-0af5-4d1f-bdcf-fb75b18d8130, location=US}\n",
      "23/06/24 15:53:46 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/442 using temp file gs://kafka-spark-data/spark-metadata/commits/.442.b3ee3060-943a-4a66-9cb7-6f54ae03c889.tmp\n",
      "23/06/24 15:53:47 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.442.b3ee3060-943a-4a66-9cb7-6f54ae03c889.tmp to gs://kafka-spark-data/spark-metadata/commits/442\n",
      "23/06/24 15:53:47 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:53:33.676Z\",\n",
      "  \"batchId\" : 442,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.21944261575597981,\n",
      "  \"processedRowsPerSecond\" : 0.21045247281655557,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 10250,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 14255,\n",
      "    \"walCommit\" : 2400\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5382\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5385\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.21944261575597981,\n",
      "    \"processedRowsPerSecond\" : 0.21045247281655557\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:53:47 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 14256 milliseconds\n",
      "23/06/24 15:53:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5387.\n",
      "23/06/24 15:53:48 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/443 using temp file gs://kafka-spark-data/spark-metadata/offsets/.443.0b9fd6ef-b591-4e98-935c-40370c8a1af6.tmp\n",
      "23/06/24 15:53:49 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.443.0b9fd6ef-b591-4e98-935c-40370c8a1af6.tmp to gs://kafka-spark-data/spark-metadata/offsets/443\n",
      "23/06/24 15:53:49 INFO MicroBatchExecution: Committed offsets for batch 443. Metadata OffsetSeqMetadata(0,1687640027934,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:53:50 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:50 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:50 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:50 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:53:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:53:50 INFO DAGScheduler: Got job 240 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:53:50 INFO DAGScheduler: Final stage: ResultStage 240 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:53:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:53:50 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:53:50 INFO DAGScheduler: Submitting ResultStage 240 (MapPartitionsRDD[1686] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:53:50 INFO MemoryStore: Block broadcast_240 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:53:50 INFO MemoryStore: Block broadcast_240_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:53:50 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:53:50 INFO SparkContext: Created broadcast 240 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:53:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 240 (MapPartitionsRDD[1686] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:53:50 INFO TaskSchedulerImpl: Adding task set 240.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:53:50 INFO TaskSetManager: Starting task 0.0 in stage 240.0 (TID 240) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:53:50 INFO Executor: Running task 0.0 in stage 240.0 (TID 240)\n",
      "23/06/24 15:53:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:53:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:53:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:53:51 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:53:51 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:53:51 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:53:51 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:53:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:53:51 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:53:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:53:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5385 for partition ticketmaster-0\n",
      "23/06/24 15:53:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 240:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:53:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:53:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5388.\n",
      "23/06/24 15:53:53 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-900e5a0e-5503-4f57-95ba-8fd0f4fa1b7a/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:53:53 INFO FileOutputCommitter: Saved output of task 'attempt_202306241553503275691611424142084_0240_m_000000_240' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-900e5a0e-5503-4f57-95ba-8fd0f4fa1b7a/_temporary/0/task_202306241553503275691611424142084_0240_m_000000\n",
      "23/06/24 15:53:53 INFO SparkHadoopMapRedUtil: attempt_202306241553503275691611424142084_0240_m_000000_240: Committed. Elapsed time: 1011 ms.\n",
      "23/06/24 15:53:53 INFO Executor: Finished task 0.0 in stage 240.0 (TID 240). 2579 bytes result sent to driver\n",
      "23/06/24 15:53:53 INFO TaskSetManager: Finished task 0.0 in stage 240.0 (TID 240) in 2279 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:53:53 INFO DAGScheduler: ResultStage 240 (start at NativeMethodAccessorImpl.java:0) finished in 2.298 s\n",
      "23/06/24 15:53:53 INFO DAGScheduler: Job 240 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:53:53 INFO TaskSchedulerImpl: Removed TaskSet 240.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:53:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 240: Stage finished\n",
      "23/06/24 15:53:53 INFO DAGScheduler: Job 240 finished: start at NativeMethodAccessorImpl.java:0, took 2.298579 s\n",
      "23/06/24 15:53:53 INFO FileFormatWriter: Start to commit write Job 2d6a6d3c-09db-49dc-b743-ef8243e25e7c.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:53:54 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-900e5a0e-5503-4f57-95ba-8fd0f4fa1b7a/_temporary/0/task_202306241553503275691611424142084_0240_m_000000/' directory.\n",
      "23/06/24 15:53:54 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-900e5a0e-5503-4f57-95ba-8fd0f4fa1b7a/' directory.\n",
      "23/06/24 15:53:55 INFO BlockManagerInfo: Removed broadcast_240_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:53:55 INFO FileFormatWriter: Write Job 2d6a6d3c-09db-49dc-b743-ef8243e25e7c committed. Elapsed time: 2222 ms.\n",
      "23/06/24 15:53:55 INFO FileFormatWriter: Finished processing stats for write job 2d6a6d3c-09db-49dc-b743-ef8243e25e7c.\n",
      "23/06/24 15:53:55 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-900e5a0e-5503-4f57-95ba-8fd0f4fa1b7a/part-00000-86cc415c-0c32-4d4f-97a5-3e005dd77d95-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=1f35d808-b9ba-430e-ac70-fd3e9fe3f9f4, location=US}\n",
      "23/06/24 15:53:57 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=1f35d808-b9ba-430e-ac70-fd3e9fe3f9f4, location=US}\n",
      "23/06/24 15:53:58 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/443 using temp file gs://kafka-spark-data/spark-metadata/commits/.443.f6ca273a-4a6c-40f2-961c-c41c5aa6a527.tmp\n",
      "23/06/24 15:53:59 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.443.f6ca273a-4a6c-40f2-961c-c41c5aa6a527.tmp to gs://kafka-spark-data/spark-metadata/commits/443\n",
      "23/06/24 15:53:59 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:53:47.932Z\",\n",
      "  \"batchId\" : 443,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.14029180695847362,\n",
      "  \"processedRowsPerSecond\" : 0.17151187719749594,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 7419,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 11661,\n",
      "    \"walCommit\" : 2427\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5385\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5387\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.14029180695847362,\n",
      "    \"processedRowsPerSecond\" : 0.17151187719749594\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:53:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11664 milliseconds\n",
      "23/06/24 15:53:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:53:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5390.\n",
      "23/06/24 15:54:00 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/444 using temp file gs://kafka-spark-data/spark-metadata/offsets/.444.d2bce831-044c-4ad7-bcfe-7437085aaa84.tmp\n",
      "23/06/24 15:54:01 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.444.d2bce831-044c-4ad7-bcfe-7437085aaa84.tmp to gs://kafka-spark-data/spark-metadata/offsets/444\n",
      "23/06/24 15:54:01 INFO MicroBatchExecution: Committed offsets for batch 444. Metadata OffsetSeqMetadata(0,1687640039607,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:54:02 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:02 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:02 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:02 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:54:03 INFO DAGScheduler: Got job 241 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:54:03 INFO DAGScheduler: Final stage: ResultStage 241 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:54:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:54:03 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:54:03 INFO DAGScheduler: Submitting ResultStage 241 (MapPartitionsRDD[1693] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:54:03 INFO MemoryStore: Block broadcast_241 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:54:03 INFO MemoryStore: Block broadcast_241_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 434.0 MiB)\n",
      "23/06/24 15:54:03 INFO BlockManagerInfo: Added broadcast_241_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:54:03 INFO SparkContext: Created broadcast 241 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:54:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 241 (MapPartitionsRDD[1693] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:54:03 INFO TaskSchedulerImpl: Adding task set 241.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:54:03 INFO TaskSetManager: Starting task 0.0 in stage 241.0 (TID 241) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:54:03 INFO Executor: Running task 0.0 in stage 241.0 (TID 241)\n",
      "23/06/24 15:54:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:03 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:54:03 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:54:03 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:54:03 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:54:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:54:03 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:54:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:54:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5388 for partition ticketmaster-0\n",
      "23/06/24 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 241:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5391.\n",
      "23/06/24 15:54:05 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-eba74d4b-ec03-4f61-9858-b56734d1cf03/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:54:05 INFO FileOutputCommitter: Saved output of task 'attempt_202306241554037237758450032569003_0241_m_000000_241' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-eba74d4b-ec03-4f61-9858-b56734d1cf03/_temporary/0/task_202306241554037237758450032569003_0241_m_000000\n",
      "23/06/24 15:54:05 INFO SparkHadoopMapRedUtil: attempt_202306241554037237758450032569003_0241_m_000000_241: Committed. Elapsed time: 1175 ms.\n",
      "23/06/24 15:54:05 INFO Executor: Finished task 0.0 in stage 241.0 (TID 241). 2579 bytes result sent to driver\n",
      "23/06/24 15:54:05 INFO TaskSetManager: Finished task 0.0 in stage 241.0 (TID 241) in 2459 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:54:05 INFO TaskSchedulerImpl: Removed TaskSet 241.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:54:05 INFO DAGScheduler: ResultStage 241 (start at NativeMethodAccessorImpl.java:0) finished in 2.483 s\n",
      "23/06/24 15:54:05 INFO DAGScheduler: Job 241 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:54:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 241: Stage finished\n",
      "23/06/24 15:54:05 INFO DAGScheduler: Job 241 finished: start at NativeMethodAccessorImpl.java:0, took 2.483973 s\n",
      "23/06/24 15:54:05 INFO FileFormatWriter: Start to commit write Job 3da2b4f6-2d7d-4fa2-a740-084a138b6577.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:54:06 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-eba74d4b-ec03-4f61-9858-b56734d1cf03/_temporary/0/task_202306241554037237758450032569003_0241_m_000000/' directory.\n",
      "23/06/24 15:54:07 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-eba74d4b-ec03-4f61-9858-b56734d1cf03/' directory.\n",
      "23/06/24 15:54:07 INFO FileFormatWriter: Write Job 3da2b4f6-2d7d-4fa2-a740-084a138b6577 committed. Elapsed time: 2365 ms.\n",
      "23/06/24 15:54:07 INFO FileFormatWriter: Finished processing stats for write job 3da2b4f6-2d7d-4fa2-a740-084a138b6577.\n",
      "23/06/24 15:54:08 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-eba74d4b-ec03-4f61-9858-b56734d1cf03/part-00000-a19da383-5089-43f5-a73d-fcfc3d5ac30f-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=91471fa7-c5fb-45de-b1d5-8cfbf2c11cf5, location=US}\n",
      "23/06/24 15:54:11 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=91471fa7-c5fb-45de-b1d5-8cfbf2c11cf5, location=US}\n",
      "23/06/24 15:54:13 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/444 using temp file gs://kafka-spark-data/spark-metadata/commits/.444.b6d5b753-6de3-4c07-90de-c68f4ca5a9ad.tmp\n",
      "23/06/24 15:54:14 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.444.b6d5b753-6de3-4c07-90de-c68f4ca5a9ad.tmp to gs://kafka-spark-data/spark-metadata/commits/444\n",
      "23/06/24 15:54:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:53:59.596Z\",\n",
      "  \"batchId\" : 444,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.257201646090535,\n",
      "  \"processedRowsPerSecond\" : 0.20072260136491368,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 10535,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 11,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 14946,\n",
      "    \"walCommit\" : 2566\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5387\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5390\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.257201646090535,\n",
      "    \"processedRowsPerSecond\" : 0.20072260136491368\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:54:14 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 14953 milliseconds\n",
      "23/06/24 15:54:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:54:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5393.\n",
      "23/06/24 15:54:14 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/445 using temp file gs://kafka-spark-data/spark-metadata/offsets/.445.001a0664-dd20-4829-9312-b79f35146a01.tmp\n",
      "23/06/24 15:54:15 INFO BlockManagerInfo: Removed broadcast_241_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:54:16 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.445.001a0664-dd20-4829-9312-b79f35146a01.tmp to gs://kafka-spark-data/spark-metadata/offsets/445\n",
      "23/06/24 15:54:16 INFO MicroBatchExecution: Committed offsets for batch 445. Metadata OffsetSeqMetadata(0,1687640054562,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:54:17 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:17 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:17 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:17 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:54:18 INFO DAGScheduler: Got job 242 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:54:18 INFO DAGScheduler: Final stage: ResultStage 242 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:54:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:54:18 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:54:18 INFO DAGScheduler: Submitting ResultStage 242 (MapPartitionsRDD[1700] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:54:18 INFO MemoryStore: Block broadcast_242 stored as values in memory (estimated size 295.4 KiB, free 434.1 MiB)\n",
      "23/06/24 15:54:18 INFO MemoryStore: Block broadcast_242_piece0 stored as bytes in memory (estimated size 95.0 KiB, free 434.0 MiB)\n",
      "23/06/24 15:54:18 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:54:18 INFO SparkContext: Created broadcast 242 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:54:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 242 (MapPartitionsRDD[1700] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:54:18 INFO TaskSchedulerImpl: Adding task set 242.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:54:18 INFO TaskSetManager: Starting task 0.0 in stage 242.0 (TID 242) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:54:18 INFO Executor: Running task 0.0 in stage 242.0 (TID 242)\n",
      "23/06/24 15:54:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:18 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:54:18 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:54:18 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:54:18 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:54:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:54:18 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:54:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:54:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5391 for partition ticketmaster-0\n",
      "23/06/24 15:54:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 242:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:54:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:54:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:54:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5394.\n",
      "23/06/24 15:54:20 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e66b52c5-8a04-4dda-a4b0-ba63bec5d534/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:54:20 INFO FileOutputCommitter: Saved output of task 'attempt_20230624155418287911688369781841_0242_m_000000_242' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e66b52c5-8a04-4dda-a4b0-ba63bec5d534/_temporary/0/task_20230624155418287911688369781841_0242_m_000000\n",
      "23/06/24 15:54:20 INFO SparkHadoopMapRedUtil: attempt_20230624155418287911688369781841_0242_m_000000_242: Committed. Elapsed time: 964 ms.\n",
      "23/06/24 15:54:20 INFO Executor: Finished task 0.0 in stage 242.0 (TID 242). 2536 bytes result sent to driver\n",
      "23/06/24 15:54:20 INFO TaskSetManager: Finished task 0.0 in stage 242.0 (TID 242) in 2265 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:54:20 INFO TaskSchedulerImpl: Removed TaskSet 242.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:54:20 INFO DAGScheduler: ResultStage 242 (start at NativeMethodAccessorImpl.java:0) finished in 2.291 s\n",
      "23/06/24 15:54:20 INFO DAGScheduler: Job 242 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:54:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 242: Stage finished\n",
      "23/06/24 15:54:20 INFO DAGScheduler: Job 242 finished: start at NativeMethodAccessorImpl.java:0, took 2.293342 s\n",
      "23/06/24 15:54:20 INFO FileFormatWriter: Start to commit write Job c125a5a2-bd37-493c-b04d-c44314a9b97f.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:54:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e66b52c5-8a04-4dda-a4b0-ba63bec5d534/_temporary/0/task_20230624155418287911688369781841_0242_m_000000/' directory.\n",
      "23/06/24 15:54:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e66b52c5-8a04-4dda-a4b0-ba63bec5d534/' directory.\n",
      "23/06/24 15:54:22 INFO FileFormatWriter: Write Job c125a5a2-bd37-493c-b04d-c44314a9b97f committed. Elapsed time: 2237 ms.\n",
      "23/06/24 15:54:22 INFO FileFormatWriter: Finished processing stats for write job c125a5a2-bd37-493c-b04d-c44314a9b97f.\n",
      "23/06/24 15:54:23 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-e66b52c5-8a04-4dda-a4b0-ba63bec5d534/part-00000-0004461a-74c5-43a1-a4b8-2ed5a648a588-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=a17c29f0-1796-4450-80db-ed8d5cd99c83, location=US}\n",
      "23/06/24 15:54:25 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=a17c29f0-1796-4450-80db-ed8d5cd99c83, location=US}\n",
      "23/06/24 15:54:26 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/445 using temp file gs://kafka-spark-data/spark-metadata/commits/.445.e4fc4539-c04c-472e-84f6-368ad2cdcbc9.tmp\n",
      "23/06/24 15:54:28 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.445.e4fc4539-c04c-472e-84f6-368ad2cdcbc9.tmp to gs://kafka-spark-data/spark-metadata/commits/445\n",
      "23/06/24 15:54:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:54:14.549Z\",\n",
      "  \"batchId\" : 445,\n",
      "  \"numInputRows\" : 3,\n",
      "  \"inputRowsPerSecond\" : 0.20062863639403464,\n",
      "  \"processedRowsPerSecond\" : 0.22223868434698865,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 8846,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 13,\n",
      "    \"queryPlanning\" : 10,\n",
      "    \"triggerExecution\" : 13499,\n",
      "    \"walCommit\" : 2487\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5390\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5393\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 3,\n",
      "    \"inputRowsPerSecond\" : 0.20062863639403464,\n",
      "    \"processedRowsPerSecond\" : 0.22223868434698865\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:54:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 13501 milliseconds\n",
      "23/06/24 15:54:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:54:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Resetting offset for partition ticketmaster-0 to offset 5395.\n",
      "23/06/24 15:54:28 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/offsets/446 using temp file gs://kafka-spark-data/spark-metadata/offsets/.446.bd31a92d-3b1e-4de9-b154-eff7564b1238.tmp\n",
      "23/06/24 15:54:30 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/offsets/.446.bd31a92d-3b1e-4de9-b154-eff7564b1238.tmp to gs://kafka-spark-data/spark-metadata/offsets/446\n",
      "23/06/24 15:54:30 INFO MicroBatchExecution: Committed offsets for batch 446. Metadata OffsetSeqMetadata(0,1687640068060,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/24 15:54:30 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:30 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:30 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:30 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/24 15:54:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/24 15:54:31 INFO DAGScheduler: Got job 243 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/24 15:54:31 INFO DAGScheduler: Final stage: ResultStage 243 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/24 15:54:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/24 15:54:31 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/24 15:54:31 INFO DAGScheduler: Submitting ResultStage 243 (MapPartitionsRDD[1707] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/24 15:54:31 INFO MemoryStore: Block broadcast_243 stored as values in memory (estimated size 295.4 KiB, free 433.7 MiB)\n",
      "23/06/24 15:54:31 INFO MemoryStore: Block broadcast_243_piece0 stored as bytes in memory (estimated size 94.9 KiB, free 433.6 MiB)\n",
      "23/06/24 15:54:31 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on nics-mbp.attlocal.net:52028 (size: 94.9 KiB, free: 434.2 MiB)\n",
      "23/06/24 15:54:31 INFO SparkContext: Created broadcast 243 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/24 15:54:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 243 (MapPartitionsRDD[1707] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/24 15:54:31 INFO TaskSchedulerImpl: Adding task set 243.0 with 1 tasks resource profile 0\n",
      "23/06/24 15:54:31 INFO TaskSetManager: Starting task 0.0 in stage 243.0 (TID 243) (nics-mbp.attlocal.net, executor driver, partition 0, PROCESS_LOCAL, 5398 bytes) taskResourceAssignments Map()\n",
      "23/06/24 15:54:31 INFO Executor: Running task 0.0 in stage 243.0 (TID 243)\n",
      "23/06/24 15:54:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/24 15:54:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/24 15:54:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/06/24 15:54:31 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:54:31 INFO CodecConfig: Compression: SNAPPY\n",
      "23/06/24 15:54:31 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/06/24 15:54:31 INFO ParquetOutputFormat: Validation is off\n",
      "23/06/24 15:54:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/06/24 15:54:31 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "23/06/24 15:54:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_zipcode\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venues_timezone\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_full\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_state_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_country_short\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_address\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_longitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"venue_latitude\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_segment_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_genre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"attraction_subgenre_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_date\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ticket_status\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_start_time\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"currency\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_price\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_name (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_url (STRING);\n",
      "  optional binary venue_name (STRING);\n",
      "  optional binary venue_id (STRING);\n",
      "  optional binary venue_zipcode (STRING);\n",
      "  optional binary venues_timezone (STRING);\n",
      "  optional binary venue_city (STRING);\n",
      "  optional binary venue_state_full (STRING);\n",
      "  optional binary venue_state_short (STRING);\n",
      "  optional binary venue_country_name (STRING);\n",
      "  optional binary venue_country_short (STRING);\n",
      "  optional binary venue_address (STRING);\n",
      "  optional binary venue_longitude (STRING);\n",
      "  optional binary venue_latitude (STRING);\n",
      "  optional binary attraction_name (STRING);\n",
      "  optional binary attraction_type (STRING);\n",
      "  optional binary attraction_id (STRING);\n",
      "  optional binary attraction_url (STRING);\n",
      "  optional binary attraction_segment_id (STRING);\n",
      "  optional binary attraction_segment_name (STRING);\n",
      "  optional binary attraction_genre_id (STRING);\n",
      "  optional binary attraction_genre_name (STRING);\n",
      "  optional binary attraction_subgenre_id (STRING);\n",
      "  optional binary attraction_subgenre_name (STRING);\n",
      "  optional binary event_start_date (STRING);\n",
      "  optional binary ticket_status (STRING);\n",
      "  optional binary event_start_time (STRING);\n",
      "  optional binary currency (STRING);\n",
      "  optional binary min_price (STRING);\n",
      "  optional binary max_price (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "23/06/24 15:54:32 INFO BlockManagerInfo: Removed broadcast_242_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 95.0 KiB, free: 434.3 MiB)\n",
      "23/06/24 15:54:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to offset 5394 for partition ticketmaster-0\n",
      "23/06/24 15:54:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 243:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:54:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/24 15:54:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/24 15:54:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor-7, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-executor] Resetting offset for partition ticketmaster-0 to offset 5396.\n",
      "23/06/24 15:54:33 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Group coordinator 127.0.0.1:9092 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery\n",
      "23/06/24 15:54:34 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7f4a908b-c60a-485d-90dc-974263385e0a/_temporary/0/_temporary/' directory.\n",
      "23/06/24 15:54:34 INFO FileOutputCommitter: Saved output of task 'attempt_202306241554313259584058231161412_0243_m_000000_243' to gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7f4a908b-c60a-485d-90dc-974263385e0a/_temporary/0/task_202306241554313259584058231161412_0243_m_000000\n",
      "23/06/24 15:54:34 INFO SparkHadoopMapRedUtil: attempt_202306241554313259584058231161412_0243_m_000000_243: Committed. Elapsed time: 961 ms.\n",
      "23/06/24 15:54:34 INFO Executor: Finished task 0.0 in stage 243.0 (TID 243). 2579 bytes result sent to driver\n",
      "23/06/24 15:54:34 INFO TaskSetManager: Finished task 0.0 in stage 243.0 (TID 243) in 2283 ms on nics-mbp.attlocal.net (executor driver) (1/1)\n",
      "23/06/24 15:54:34 INFO TaskSchedulerImpl: Removed TaskSet 243.0, whose tasks have all completed, from pool \n",
      "23/06/24 15:54:34 INFO DAGScheduler: ResultStage 243 (start at NativeMethodAccessorImpl.java:0) finished in 2.302 s\n",
      "23/06/24 15:54:34 INFO DAGScheduler: Job 243 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/24 15:54:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 243: Stage finished\n",
      "23/06/24 15:54:34 INFO DAGScheduler: Job 243 finished: start at NativeMethodAccessorImpl.java:0, took 2.304270 s\n",
      "23/06/24 15:54:34 INFO FileFormatWriter: Start to commit write Job 45e552ce-bdb9-4549-ab08-e37154ad9269.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/24 15:54:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:35 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7f4a908b-c60a-485d-90dc-974263385e0a/_temporary/0/task_202306241554313259584058231161412_0243_m_000000/' directory.\n",
      "23/06/24 15:54:35 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7f4a908b-c60a-485d-90dc-974263385e0a/' directory.\n",
      "23/06/24 15:54:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:36 INFO BlockManagerInfo: Removed broadcast_243_piece0 on nics-mbp.attlocal.net:52028 in memory (size: 94.9 KiB, free: 434.4 MiB)\n",
      "23/06/24 15:54:36 INFO FileFormatWriter: Write Job 45e552ce-bdb9-4549-ab08-e37154ad9269 committed. Elapsed time: 2670 ms.\n",
      "23/06/24 15:54:36 INFO FileFormatWriter: Finished processing stats for write job 45e552ce-bdb9-4549-ab08-e37154ad9269.\n",
      "23/06/24 15:54:37 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=twitter_kafka_pyspark_test, projectId=global-maxim-338114, tableId=kafka_pyspark}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=event_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_zipcode, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venues_timezone, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_city, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_full, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_state_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_country_short, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_address, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_longitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=venue_latitude, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_type, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_url, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_segment_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_genre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=attraction_subgenre_name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_date, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=ticket_status, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=event_start_time, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=currency, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_price, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://kafka-spark-data/.spark-bigquery-local-1687636163344-7f4a908b-c60a-485d-90dc-974263385e0a/part-00000-230b9e58-f180-44a7-8336-71bae76e09a7-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=global-maxim-338114, job=2fa1bd82-1e57-4d07-83aa-65d12cb8d4e4, location=US}\n",
      "23/06/24 15:54:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:40 INFO BigQueryClient: Done loading to global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark. jobId: JobId{project=global-maxim-338114, job=2fa1bd82-1e57-4d07-83aa-65d12cb8d4e4, location=US}\n",
      "23/06/24 15:54:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:41 INFO CheckpointFileManager: Writing atomically to gs://kafka-spark-data/spark-metadata/commits/446 using temp file gs://kafka-spark-data/spark-metadata/commits/.446.b883240d-4a39-4ff9-8c0c-13830fa6c5f0.tmp\n",
      "23/06/24 15:54:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:43 INFO CheckpointFileManager: Renamed temp file gs://kafka-spark-data/spark-metadata/commits/.446.b883240d-4a39-4ff9-8c0c-13830fa6c5f0.tmp to gs://kafka-spark-data/spark-metadata/commits/446\n",
      "23/06/24 15:54:43 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"f2bc1daa-c744-4e59-9ab7-7162c43fc781\",\n",
      "  \"runId\" : \"760626d0-8b25-4576-a079-b96837962fdd\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-24T20:54:28.050Z\",\n",
      "  \"batchId\" : 446,\n",
      "  \"numInputRows\" : 2,\n",
      "  \"inputRowsPerSecond\" : 0.1481371750240723,\n",
      "  \"processedRowsPerSecond\" : 0.13319126265316994,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 10531,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 10,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 15016,\n",
      "    \"walCommit\" : 2694\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5393\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 5395\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 2,\n",
      "    \"inputRowsPerSecond\" : 0.1481371750240723,\n",
      "    \"processedRowsPerSecond\" : 0.13319126265316994\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"BigQueryStreamingSink(org.apache.spark.sql.SQLContext@6be89169,Map(checkpointlocation -> gs://kafka-spark-data/spark-metadata, temporarygcsbucket -> kafka-spark-data, table -> global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark, mode -> FAILFAST),List(),Append,com.google.cloud.spark.bigquery.SparkBigQueryConfig@5a35208e,com.google.cloud.bigquery.connector.common.BigQueryClient@16c104bf)\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "23/06/24 15:54:43 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 15020 milliseconds\n",
      "23/06/24 15:54:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:51 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:54 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:55 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:56 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:54:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/06/24 15:55:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0-6, groupId=spark-kafka-source-ba1563fc-d255-476e-bc57-dbb77866df2e--2001384727-driver-0] Connection to node 1 (/127.0.0.1:9092) could not be established. Broker may not be available.\n"
     ]
    }
   ],
   "source": [
    "##### STREAMING DATA PROCESSING #####\n",
    "\n",
    "# Read the data from kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    \n",
    "# Apply deserialization or further processing if needed\n",
    "df1 = df.withColumn(\"parsed_data\", from_json(\"value\", schema))\n",
    "\n",
    "### DATA TYPE CONVERSIONS ####\n",
    "# Extract and convert the \"venue_zipcode\" as Integer\n",
    "# df1 = df1.withColumn(\"venue_zipcode\", col(\"parsed_data.venue_zipcode\").cast(IntegerType()))\n",
    "# # Extract and convert the coordinates as Double\n",
    "# df1 = df1.withColumn(\"venue_longitude\", col(\"parsed_data.venue_longitude\").cast(DoubleType()))\n",
    "# df1 = df1.withColumn(\"venue_latitude\", col(\"parsed_data.venue_latitude\").cast(DoubleType()))\n",
    "# # Extract and Convert the event_start_date as Date \n",
    "# df1 = df1.withColumn(\"event_start_date\", col(\"parsed_data.event_start_date\").cast(DateType()))\n",
    "\n",
    "df2 = df1.select(\"parsed_data.*\")\n",
    "\n",
    "\n",
    "df2.printSchema()\n",
    "\n",
    "path = '/Users/nicburkett/Desktop/spark_output'\n",
    "\n",
    "\n",
    "# # # Write to a local file\n",
    "# # file_query = df2.writeStream \\\n",
    "# #     .format(\"csv\") \\\n",
    "# #     .outputMode(\"append\") \\\n",
    "# #     .option(\"header\", \"true\") \\\n",
    "# #     .option(\"checkpointLocation\", path) \\\n",
    "# #     .trigger(processingTime=\"10 seconds\") \\\n",
    "# #     .start(path)\n",
    "\n",
    "# # # WRITE TO GCS BUCKET \n",
    "# gcs_write = df2.writeStream \\\n",
    "#     .format(\"csv\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"path\",\"gs://kafka-spark-data/raw-spark-data\") \\\n",
    "#     .option(\"checkpointLocation\", \"gs://kafka-spark-data/spark-metadata\") \\\n",
    "#     .trigger(processingTime=\"10 seconds\") \\\n",
    "#     .start() \n",
    "\n",
    "# gcs_write.awaitTermination()\n",
    "\n",
    "# WRITE TO CONSOLE TO LOG \n",
    "# console_query = df2.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .trigger(processingTime=\"10 seconds\") \\\n",
    "#     .start() \\\n",
    "#     .awaitTermination()\n",
    "\n",
    "    # .foreachBatch(write_batch) \\\n",
    "\n",
    "gcs_bigquery_stream = df2.writeStream \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .option(\"checkpointLocation\", gcs_metadata_folder) \\\n",
    "    .option(\"temporaryGcsBucket\", 'kafka-spark-data') \\\n",
    "    .option(\"table\",'global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark') \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .start()\n",
    "\n",
    "    # .option(\"failOnDataLoss\",'false') \\\n",
    "\n",
    "gcs_bigquery_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/22 20:39:55 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/22 20:39:55 INFO ResolveWriteToStream: Checkpoint root /private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c resolved to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c.\n",
      "23/06/22 20:39:55 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/22 20:39:55 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/metadata using temp file file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/.metadata.db73f13d-0540-4f6f-bd8a-73b3b535ef60.tmp\n",
      "23/06/22 20:39:55 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/.metadata.db73f13d-0540-4f6f-bd8a-73b3b535ef60.tmp to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/metadata\n",
      "23/06/22 20:39:55 INFO MicroBatchExecution: Starting [id = 5ff3f0d9-8649-4084-a0de-9c984d47f474, runId = 061d6392-20e9-47dc-a164-da2a57f51557]. Use file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c to store the query checkpoint.\n",
      "23/06/22 20:39:55 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3ffa2c25] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@313dc472]\n",
      "23/06/22 20:39:55 INFO MicroBatchExecution: Starting new streaming query.\n",
      "23/06/22 20:39:55 INFO MicroBatchExecution: Stream started from {}\n",
      "23/06/22 20:39:55 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.offset.reset = earliest\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 1\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "23/06/22 20:39:55 INFO AppInfoParser: Kafka version: 2.6.0\n",
      "23/06/22 20:39:55 INFO AppInfoParser: Kafka commitId: 62abe01bee039651\n",
      "23/06/22 20:39:55 INFO AppInfoParser: Kafka startTimeMs: 1687484395268\n",
      "23/06/22 20:39:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Subscribed to topic(s): ticketmaster\n",
      "23/06/22 20:39:55 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Cluster ID: kJYfpPSDQYKOEac6knVYNg\n",
      "23/06/22 20:39:55 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Discovered group coordinator 127.0.0.1:9092 (id: 2147483646 rack: null)\n",
      "23/06/22 20:39:55 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] (Re-)joining group\n",
      "23/06/22 20:39:55 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.\n",
      "23/06/22 20:39:55 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] (Re-)joining group\n",
      "23/06/22 20:39:58 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Finished assignment for group at generation 1: {consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33-d918989d-4970-4a0d-ba8b-2562fc284fa0=Assignment(partitions=[ticketmaster-0])}\n",
      "23/06/22 20:39:58 INFO AbstractCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Successfully joined group with generation 1\n",
      "23/06/22 20:39:58 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Notifying assignor about the new Assignment(partitions=[ticketmaster-0])\n",
      "23/06/22 20:39:58 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Adding newly assigned partitions: ticketmaster-0\n",
      "23/06/22 20:39:58 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Found no committed offset for partition ticketmaster-0\n",
      "23/06/22 20:39:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 0.\n",
      "23/06/22 20:39:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:39:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:39:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/sources/0/0 using temp file file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/sources/0/.0.b96c1db1-6d54-4d9e-97a6-2e84101f2c29.tmp\n",
      "23/06/22 20:39:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/sources/0/.0.b96c1db1-6d54-4d9e-97a6-2e84101f2c29.tmp to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/sources/0/0\n",
      "23/06/22 20:39:58 INFO KafkaMicroBatchStream: Initial offsets: {\"ticketmaster\":{\"0\":1393}}\n",
      "23/06/22 20:39:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:39:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:39:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/offsets/0 using temp file file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/offsets/.0.904f4124-92b7-4b34-8879-a7b63ce929a5.tmp\n",
      "23/06/22 20:39:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/offsets/.0.904f4124-92b7-4b34-8879-a7b63ce929a5.tmp to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/offsets/0\n",
      "23/06/22 20:39:58 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1687484398424,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "23/06/22 20:39:58 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/22 20:39:58 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/22 20:39:58 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/22 20:39:58 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/22 20:39:58 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/22 20:39:58 INFO KafkaOffsetReaderConsumer: Partitions added: Map()\n",
      "23/06/22 20:39:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@7349b08. The input RDD has 1 partitions.\n",
      "23/06/22 20:39:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "23/06/22 20:39:58 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/22 20:39:58 INFO DAGScheduler: Final stage: ResultStage 13 (start at NativeMethodAccessorImpl.java:0)\n",
      "23/06/22 20:39:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/22 20:39:58 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/22 20:39:58 INFO DAGScheduler: Submitting ResultStage 13 (ParallelCollectionRDD[113] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/22 20:39:58 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.2 KiB, free 434.0 MiB)\n",
      "23/06/22 20:39:58 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1987.0 B, free 434.0 MiB)\n",
      "23/06/22 20:39:58 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.4.24:51040 (size: 1987.0 B, free: 434.3 MiB)\n",
      "23/06/22 20:39:58 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
      "23/06/22 20:39:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (ParallelCollectionRDD[113] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/22 20:39:58 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "23/06/22 20:39:58 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (192.168.4.24, executor driver, partition 0, PROCESS_LOCAL, 4474 bytes) taskResourceAssignments Map()\n",
      "23/06/22 20:39:58 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)\n",
      "23/06/22 20:39:58 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 12, attempt 0, stage 13.0)\n",
      "23/06/22 20:39:58 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 13.0)\n",
      "23/06/22 20:39:58 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 1119 bytes result sent to driver\n",
      "23/06/22 20:39:58 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 3 ms on 192.168.4.24 (executor driver) (1/1)\n",
      "23/06/22 20:39:58 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "23/06/22 20:39:58 INFO DAGScheduler: ResultStage 13 (start at NativeMethodAccessorImpl.java:0) finished in 0.008 s\n",
      "23/06/22 20:39:58 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/22 20:39:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "23/06/22 20:39:58 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 0.008294 s\n",
      "23/06/22 20:39:58 INFO WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@7349b08 is committing.\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+----------+--------+---------+----------+--------+-------------+---------------+----------+----------------+-----------------+------------------+-------------------+-------------+---------------+--------------+---------------+---------------+-------------+--------------+---------------------+-----------------------+-------------------+---------------------+----------------------+------------------------+----------------+-------------+----------------+--------+---------+---------+\n",
      "|event_name|event_type|event_id|event_url|venue_name|venue_id|venue_zipcode|venues_timezone|venue_city|venue_state_full|venue_state_short|venue_country_name|venue_country_short|venue_address|venue_longitude|venue_latitude|attraction_name|attraction_type|attraction_id|attraction_url|attraction_segment_id|attraction_segment_name|attraction_genre_id|attraction_genre_name|attraction_subgenre_id|attraction_subgenre_name|event_start_date|ticket_status|event_start_time|currency|min_price|max_price|\n",
      "+----------+----------+--------+---------+----------+--------+-------------+---------------+----------+----------------+-----------------+------------------+-------------------+-------------+---------------+--------------+---------------+---------------+-------------+--------------+---------------------+-----------------------+-------------------+---------------------+----------------------+------------------------+----------------+-------------+----------------+--------+---------+---------+\n",
      "+----------+----------+--------+---------+----------+--------+-------------+---------------+----------+----------------+-----------------+------------------+-------------------+-------------+---------------+--------------+---------------+---------------+-------------+--------------+---------------------+-----------------------+-------------------+---------------------+----------------------+------------------------+----------------+-------------+----------------+--------+---------+---------+\n",
      "\n",
      "23/06/22 20:39:58 INFO WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@7349b08 committed.\n",
      "23/06/22 20:39:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/commits/0 using temp file file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/commits/.0.22f41be0-96af-4b58-8140-35d6003cf105.tmp\n",
      "23/06/22 20:39:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/commits/.0.22f41be0-96af-4b58-8140-35d6003cf105.tmp to file:/private/var/folders/gr/l9vf1gtd2_s_3tz5n15mgkn80000gn/T/temporary-7718572b-fe8e-461e-9e94-f12a490f995c/commits/0\n",
      "23/06/22 20:39:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5ff3f0d9-8649-4084-a0de-9c984d47f474\",\n",
      "  \"runId\" : \"061d6392-20e9-47dc-a164-da2a57f51557\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-23T01:39:55.254Z\",\n",
      "  \"batchId\" : 0,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 27,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 3170,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 3318,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : null,\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@1ecfbdf1\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "23/06/22 20:40:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:40:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:40:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:40:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:40:10 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5ff3f0d9-8649-4084-a0de-9c984d47f474\",\n",
      "  \"runId\" : \"061d6392-20e9-47dc-a164-da2a57f51557\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-23T01:40:10.005Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 3,\n",
      "    \"triggerExecution\" : 3\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@1ecfbdf1\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nicburkett/opt/anaconda3/envs/spark/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nicburkett/opt/anaconda3/envs/spark/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nicburkett/opt/anaconda3/envs/spark/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# WRITE TO CONSOLE TO LOG \u001b[39;00m\n\u001b[1;32m      2\u001b[0m console_query \u001b[39m=\u001b[39m df2\u001b[39m.\u001b[39;49mwriteStream \\\n\u001b[1;32m      3\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mconsole\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[39m.\u001b[39;49moutputMode(\u001b[39m\"\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[39m.\u001b[39;49mtrigger(processingTime\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m10 seconds\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[39m.\u001b[39;49mstart() \\\n\u001b[0;32m----> 7\u001b[0m     \u001b[39m.\u001b[39;49mawaitTermination()\n\u001b[1;32m      9\u001b[0m     \u001b[39m# .foreachBatch(write_batch) \\\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/pyspark/sql/streaming/query.py:201\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsq\u001b[39m.\u001b[39mawaitTermination(\u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsq\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/22 20:40:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:40:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:40:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5ff3f0d9-8649-4084-a0de-9c984d47f474\",\n",
      "  \"runId\" : \"061d6392-20e9-47dc-a164-da2a57f51557\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-23T01:40:20.005Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 10,\n",
      "    \"triggerExecution\" : 12\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@1ecfbdf1\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "23/06/22 20:40:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:40:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:40:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:40:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:40:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5ff3f0d9-8649-4084-a0de-9c984d47f474\",\n",
      "  \"runId\" : \"061d6392-20e9-47dc-a164-da2a57f51557\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-23T01:40:40.003Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 4,\n",
      "    \"triggerExecution\" : 4\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@1ecfbdf1\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "23/06/22 20:40:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:40:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:41:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:41:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:41:00 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5ff3f0d9-8649-4084-a0de-9c984d47f474\",\n",
      "  \"runId\" : \"061d6392-20e9-47dc-a164-da2a57f51557\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-23T01:41:00.004Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 11,\n",
      "    \"triggerExecution\" : 11\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@1ecfbdf1\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "23/06/22 20:41:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:41:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n",
      "23/06/22 20:41:10 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5ff3f0d9-8649-4084-a0de-9c984d47f474\",\n",
      "  \"runId\" : \"061d6392-20e9-47dc-a164-da2a57f51557\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-06-23T01:41:10.006Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"latestOffset\" : 13,\n",
      "    \"triggerExecution\" : 13\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[ticketmaster]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"ticketmaster\" : {\n",
      "        \"0\" : 1393\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@1ecfbdf1\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "23/06/22 20:41:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Seeking to LATEST offset of partition ticketmaster-0\n",
      "23/06/22 20:41:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0-33, groupId=spark-kafka-source-2b8cbc25-4bf7-465c-b61f-6e26ba34a2b9--2109907342-driver-0] Resetting offset for partition ticketmaster-0 to offset 1393.\n"
     ]
    }
   ],
   "source": [
    "# WRITE TO CONSOLE TO LOG \n",
    "console_query = df2.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n",
    "\n",
    "    # .foreachBatch(write_batch) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/19 19:50:27 INFO FetchSessionHandler: [Consumer clientId=consumer-spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor-9, groupId=spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 1001:\n",
      "org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.\n",
      "23/06/19 19:50:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor-9, groupId=spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor] Connection to node 1001 (host.docker.internal/143.244.220.150:9092) could not be established. Broker may not be available.\n",
      "23/06/19 19:50:42 INFO FetchSessionHandler: [Consumer clientId=consumer-spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor-9, groupId=spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 1001:\n",
      "org.apache.kafka.common.errors.DisconnectException\n",
      "23/06/19 19:51:12 INFO FetchSessionHandler: [Consumer clientId=consumer-spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor-9, groupId=spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 1001:\n",
      "org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.\n",
      "23/06/19 19:51:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor-9, groupId=spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor] Seeking to EARLIEST offset of partition ticketmaster-0\n",
      "23/06/19 19:51:42 INFO FetchSessionHandler: [Consumer clientId=consumer-spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor-9, groupId=spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 1001:\n",
      "org.apache.kafka.common.errors.TimeoutException: Failed to send request after 30000 ms.\n",
      "23/06/19 19:51:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor-9, groupId=spark-kafka-source-d2941ca6-4ad1-48ba-bda3-65af4267d63f--2001384727-executor] Connection to node 1001 (host.docker.internal/143.244.220.150:9092) could not be established. Broker may not be available.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## BATCH DATA PROCESSING \n",
    "\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", topic_name) \\\n",
    "  .load()\\\n",
    "  .selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Apply deserialization or further processing if needed\n",
    "df1 = df.withColumn(\"parsed_data\", from_json(\"value\", schema))\n",
    "## Select the data from the parsed_data column\n",
    "df2 = df1.select(\"parsed_data.*\")\n",
    "\n",
    "gcs_metadata_folder = \"gs://kafka-spark-data/spark-metadata\"\n",
    "gcs_data_folder = \"gs://kafka-spark-data/raw-spark-data\"\n",
    "\n",
    "print(df2.schema)\n",
    "\n",
    "## WRITE TO LOCAL STORAGE\n",
    "# gcs_write = df2.write \\\n",
    "#   .format(\"csv\") \\\n",
    "#   .option(\"checkpointLocation\", \"/Users/nicburkett/Desktop/spark_output\") \\\n",
    "#   .option(\"path\",\"/Users/nicburkett/Desktop/spark_output\") \\\n",
    "#   .mode(\"overwrite\")\\\n",
    "#   .save()\n",
    "\n",
    "##WRITE TO GCS BUCKET\n",
    "# gcs_write_newfolder = df2.write \\\n",
    "#   .format(\"parquet\") \\\n",
    "#   .option(\"checkpointLocation\", gcs_metadata_folder) \\\n",
    "#   .option(\"path\",gcs_data_folder) \\\n",
    "#   .mode(\"overwrite\")\\\n",
    "#   .save()\n",
    "\n",
    "\n",
    "dataset = 'global-maxim-338114.twitter_kafka_pyspark_test'\n",
    "table = 'twitter_kafka_pyspark_test'\n",
    "\n",
    "# Write the DataFrame to BigQuery\n",
    " ## this is the bucket where the data is stored temporarily\n",
    "df2.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"checkpointLocation\", gcs_metadata_folder) \\\n",
    "    .option(\"temporaryGcsBucket\", 'kafka-spark-data') \\\n",
    "    .option(\"table\",'global-maxim-338114.twitter_kafka_pyspark_test.kafka_pyspark') \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Function to save the data to GCS with a custom filename\n",
    "def save_to_gcs(batch_df, batch_id):\n",
    "    # Convert the batch dataframe to a pandas dataframe\n",
    "    pandas_df = batch_df.toPandas()\n",
    "\n",
    "    # Get the values of the desired columns from the first row\n",
    "    column1_value = pandas_df.loc[0, \"column1\"]\n",
    "    column2_value = pandas_df.loc[0, \"column2\"]\n",
    "\n",
    "    # Get the current time\n",
    "    current_time = pd.Timestamp.now()\n",
    "\n",
    "    # Generate the custom filename\n",
    "    filename = f\"file_{column1_value}_{column2_value}_{current_time}.parquet\"\n",
    "\n",
    "    # Save the dataframe to GCS with the custom filename\n",
    "    pandas_df.to_parquet(f\"gs://{bucket_name}/{path}/{filename}\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TYPES THOUGHOUT KAFKA SERVER\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, udf\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "topic_name = 'twitter'\n",
    "# Config\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"TwitterSentimentAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "prod = {'user_id': 19, 'recipient_id': 57, 'message': 'YbfyRHyWgjuGlzOiudEcVMLJNzqUPDvV'}\n",
    "print(type(prod))\n",
    "##convert dictionary to json string (BYTES)\n",
    "serialized_prod = json.dumps(prod).encode('utf-8')\n",
    "\n",
    "print(f'The producer dtype is {type(serialized_prod)} and output is {(serialized_prod)}')\n",
    "\n",
    "## turn from string/bytes into a dictionary again\n",
    "deserializer_function = lambda x: json.loads(x.decode('utf-8'))\n",
    "deserialized_cons = deserializer_function(serialized_prod)\n",
    "\n",
    "print(f'The producer dtype is {type(deserialized_cons)} and output is {(deserialized_cons)}')\n",
    "\n",
    "### PARSING THE JSON COMING OUT \n",
    "user_id = deserialized_cons.get('user_id')\n",
    "recipient_id = deserialized_cons.get('recipient_id')\n",
    "message = deserialized_cons.get('message')\n",
    "output_parsed = print(f'UserID: {user_id}, RecipientID: {recipient_id}, Message:{message}')\n",
    "\n",
    "\n",
    "df_pandas = pd.DataFrame([deserialized_cons])\n",
    "df_pandas\n",
    "\n",
    "\n",
    "# df_spark = spark.createDataFrame(df_pandas)\n",
    "# df_spark.show()\n",
    "\n",
    "# Create a spark schema/column headers\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"recipient_id\", IntegerType(), True),\n",
    "    StructField(\"message\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame from a single row\n",
    "# data = [(user_id, recipient_id, message)]\n",
    "df_spark = spark.createDataFrame(df_pandas,schema)\n",
    "df_spark.show()\n",
    "# df.write.csv('/path/to/output.csv', header=True, mode='overwrite')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
